{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bdfdcd-6b49-4e4c-9aa4-ca98458bca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e116989-5cb4-4959-8b3f-cdb1cbf4fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dynamic_ar_model(train_data, d=1, step=1, tau_sec=3):\n",
    "    \"\"\"\n",
    "    Complete corrected version: AR coefficients update every tau seconds (block-wise random walk).\n",
    "    Uses AR correction principle from first function: v_corrected = v_base + Σ ρ[lag] × (vt_diff - a_lag × dt)\n",
    "    \"\"\"\n",
    "    print(\"Starting DYNAMIC AR model training...\")\n",
    "    \n",
    "    vt = train_data['vt']\n",
    "    s = train_data['s']\n",
    "    dv = train_data['dv']\n",
    "    label_v = train_data['label_v']\n",
    "    id_idx = train_data['id_idx']\n",
    "    N_veh = train_data['n_vehicles']\n",
    "    \n",
    "    dt = 0.5\n",
    "    D = 5  # IDM parameters\n",
    "    DELTA = 4\n",
    "    tau = int(tau_sec / dt)  # block size\n",
    "    \n",
    "    coords = {\n",
    "        \"veh_id\": np.arange(N_veh),\n",
    "        \"ar_lag\": np.arange(d),\n",
    "        \"parameter\": np.arange(D)\n",
    "    }\n",
    "    \n",
    "    with pm.Model(coords=coords) as dynamic_ar_model:\n",
    "        # Hierarchical IDM parameter structure\n",
    "        chol, _, _ = pm.LKJCholeskyCov('chol', n=D, eta=2.0, \n",
    "                                      sd_dist=pm.Exponential.dist(2, shape=D))\n",
    "        \n",
    "        # Population mean parameters\n",
    "        log_mu_vmax = pm.Normal('log_mu_vmax', mu=0, sigma=0.1)\n",
    "        log_mu_dsafe = pm.Normal('log_mu_dsafe', mu=0, sigma=0.1)\n",
    "        log_mu_tsafe = pm.Normal('log_mu_tsafe', mu=0, sigma=0.1)\n",
    "        log_mu_amax = pm.Normal('log_mu_amax', mu=0, sigma=0.1)\n",
    "        \n",
    "        # Ratio parameter setup\n",
    "        log_ratio_mu = pm.Normal('log_ratio_mu', mu=np.log(1.0), sigma=0.1)\n",
    "        ratio_mu = pm.Deterministic('ratio_mu', pt.exp(log_ratio_mu))\n",
    "        \n",
    "        # Individual ratio differences\n",
    "        log_ratio_raw = pm.Normal('log_ratio_raw', mu=0, sigma=0.1, shape=N_veh)\n",
    "        ratio_individual = pm.Deterministic('ratio_individual', ratio_mu * pt.exp(log_ratio_raw))\n",
    "        \n",
    "        # Individual random effects\n",
    "        vals_raw_vmax = pm.Normal('vals_raw_vmax', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_dsafe = pm.Normal('vals_raw_dsafe', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_tsafe = pm.Normal('vals_raw_tsafe', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_amax = pm.Normal('vals_raw_amax', mu=0, sigma=0.1, shape=N_veh)\n",
    "        \n",
    "        # Combine into matrix form (first 4 parameters)\n",
    "        vals_raw_first4 = pm.Deterministic('vals_raw_first4', pt.stack([\n",
    "            vals_raw_vmax, vals_raw_dsafe, vals_raw_tsafe, vals_raw_amax\n",
    "        ], axis=1))\n",
    "        \n",
    "        # Calculate individual parameters for first 4 parameters\n",
    "        log_mu_first4 = pt.stack([log_mu_vmax, log_mu_dsafe, log_mu_tsafe, log_mu_amax])\n",
    "        log_parameters_first4 = pm.Deterministic('log_parameters_first4', \n",
    "                                               log_mu_first4 + pt.dot(vals_raw_first4, chol[:4, :4].T))\n",
    "        parameters_first4 = pm.Deterministic('parameters_first4', pt.exp(log_parameters_first4))\n",
    "        \n",
    "        # Correct matrix concatenation\n",
    "        amin_individual = ratio_individual * parameters_first4[:, 3]  # amin = ratio * amax\n",
    "        \n",
    "        # Combine all parameters\n",
    "        parameters = pm.Deterministic('parameters', pt.stack([\n",
    "            parameters_first4[:, 0],  # vmax\n",
    "            parameters_first4[:, 1],  # dsafe\n",
    "            parameters_first4[:, 2],  # tsafe\n",
    "            parameters_first4[:, 3],  # amax\n",
    "            amin_individual            # amin\n",
    "        ], axis=1), dims=('veh_id', 'parameter'))\n",
    "        \n",
    "        # Non-hierarchical parameters\n",
    "        s_a_list = []\n",
    "        s_v_list = []\n",
    "        for i in range(N_veh):\n",
    "            s_a_i = pm.Exponential(f's_a_{i}', lam=2000)\n",
    "            s_v_i = pm.Exponential(f's_v_{i}', lam=4000)\n",
    "            s_a_list.append(s_a_i)\n",
    "            s_v_list.append(s_v_i)\n",
    "        \n",
    "        # Dynamic AR hyperparameters\n",
    "        rho_transition = pm.Deterministic('rho_transition', pt.zeros((N_veh, d)))\n",
    "        rho_innovation = pm.HalfNormal('rho_innovation', sigma=0.001, dims=(\"veh_id\", \"ar_lag\"))\n",
    "        rho_0 = pm.Normal('rho_0', mu=0, sigma=0.4, dims=(\"veh_id\", \"ar_lag\"))\n",
    "        rho_dynamic = [None] * N_veh\n",
    "        rho_block_info = {}  # Store block information for validation\n",
    "        \n",
    "        for i in range(N_veh):\n",
    "            mask = (id_idx == i)\n",
    "            n_veh_timesteps = int(np.sum(mask))\n",
    "            if n_veh_timesteps > (d * step) + 5:\n",
    "                n_blocks = int(np.ceil(n_veh_timesteps / tau))\n",
    "                \n",
    "                rho_lag_list = []\n",
    "                rho_block_values_list = []  # Store block values for each lag\n",
    "                \n",
    "                for lag in range(d):\n",
    "                    mu_scalar = rho_transition[i, lag]\n",
    "                    sigma_scalar = rho_innovation[i, lag]\n",
    "                    init_mu = rho_0[i, lag]\n",
    "                    \n",
    "                    # block-wise initial value\n",
    "                    rho_init = pm.Normal(f'rho_init_{i}_lag{lag}', mu=init_mu, sigma=0.1)\n",
    "                    \n",
    "                    # block innovation terms\n",
    "                    if n_blocks > 1:\n",
    "                        innov_blocks = pm.Normal(\n",
    "                            f'rho_innov_blocks_{i}_lag{lag}',\n",
    "                            mu=0.0,\n",
    "                            sigma=rho_innovation[i, lag], \n",
    "                            shape=(n_blocks - 1,)\n",
    "                        )\n",
    "                        cumsum_blocks = pm.math.cumsum(innov_blocks)\n",
    "                        # Concatenate into block AR values\n",
    "                        rho_block_vals = pm.math.concatenate([pm.math.stack([rho_init]), rho_init + cumsum_blocks])\n",
    "                    else:\n",
    "                        rho_block_vals = pm.math.stack([rho_init])\n",
    "                    \n",
    "                    rho_block_values_list.append(rho_block_vals)\n",
    "                    \n",
    "                    # Expand to time series, constant within each block\n",
    "                    rho_vals = pt.repeat(rho_block_vals, tau)[:n_veh_timesteps]\n",
    "                    rho_lag_list.append(rho_vals)\n",
    "                \n",
    "                # Store block information for validation\n",
    "                rho_block_info[i] = {\n",
    "                    'n_blocks': n_blocks,\n",
    "                    'block_values': rho_block_values_list,\n",
    "                    'tau': tau\n",
    "                }\n",
    "                \n",
    "                # Stack into (n_veh_timesteps, d)\n",
    "                rho_i_stack = pm.math.stack(rho_lag_list, axis=1)\n",
    "                rho_dynamic[i] = rho_i_stack\n",
    "            else:\n",
    "                rho_dynamic[i] = None\n",
    "                rho_block_info[i] = None\n",
    "        \n",
    "        # Observation model\n",
    "        for i in range(N_veh):\n",
    "            mask = (id_idx == i)\n",
    "            if rho_dynamic[i] is not None:\n",
    "                s_veh = s[mask]\n",
    "                vt_veh = vt[mask]\n",
    "                dv_veh = dv[mask]\n",
    "                label_veh = label_v[mask]\n",
    "                n_veh = len(vt_veh)\n",
    "                \n",
    "                # Use hierarchical IDM parameters (with scaling factors)\n",
    "                vmax_i = 25 * parameters[i, 0]\n",
    "                dsafe_i = 2  * parameters[i, 1]\n",
    "                tsafe_i = 1.6 * parameters[i, 2]\n",
    "                amax_i = 1.5   * parameters[i, 3]\n",
    "                amin_i = 1.5  * parameters[i, 4]\n",
    "                \n",
    "                sn = dsafe_i + vt_veh * tsafe_i + vt_veh * dv_veh / (2 * pm.math.sqrt(amax_i * amin_i))\n",
    "                a = amax_i * (1 - (vt_veh / vmax_i) ** DELTA - (sn / s_veh) ** 2)\n",
    "                \n",
    "                # Key modification: AR correction principle from first function\n",
    "                # Base speed prediction (without AR correction)\n",
    "                mean = vt_veh + a * dt\n",
    "                \n",
    "                # Apply AR correction - speed level\n",
    "                total_ar_correction = pt.zeros_like(mean)\n",
    "                \n",
    "                rho_i_dynamic = rho_dynamic[i]\n",
    "                n = n_veh\n",
    "                \n",
    "                for lag in range(d):\n",
    "                    if n > lag + 1:\n",
    "                        # Calculate speed difference and acceleration terms\n",
    "                        vt_diff = vt_veh[lag+1:n] - vt_veh[lag:n-1]\n",
    "                        a_lag = a[lag:n-1]\n",
    "                        \n",
    "                        # AR correction term: rho * (speed difference - acceleration integral)\n",
    "                        ar_correction = rho_i_dynamic[lag+1:n, lag] * (vt_diff - a_lag * dt)\n",
    "                        \n",
    "                        # Apply correction to speed prediction (from d to end)\n",
    "                        start_idx = max(d, lag+1)\n",
    "                        if start_idx < n:\n",
    "                            correction_length = n - start_idx\n",
    "                            ar_slice = ar_correction[start_idx-lag-1:start_idx-lag-1+correction_length]\n",
    "                            total_ar_correction = pt.set_subtensor(\n",
    "                                total_ar_correction[start_idx:n],\n",
    "                                total_ar_correction[start_idx:n] + ar_slice\n",
    "                            )\n",
    "                \n",
    "                # Apply AR correction for final speed prediction\n",
    "                mean_corrected = mean + total_ar_correction\n",
    "                \n",
    "                total_sigma = pm.math.sqrt((s_a_list[i] * dt) ** 2 + s_v_list[i] ** 2)\n",
    "                pm.Normal(f'obs_{i}', mu=mean_corrected, sigma=total_sigma, observed=label_veh)\n",
    "        \n",
    "        # Sampling\n",
    "        try:\n",
    "            trace = pm.sample(draws=800, tune=800, random_seed=42, chains=2, target_accept=0.95, max_treedepth=20, return_inferencedata=True)\n",
    "            print(\"Dynamic AR model training completed!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Sampling error: {e}\")\n",
    "            print(\"Trying more conservative sampling settings...\")\n",
    "            trace = pm.sample(draws=1000, tune=1000, random_seed=42, chains=2, target_accept=0.8, max_treedepth=20, return_inferencedata=True)\n",
    "            print(\"Dynamic AR model training completed (using conservative settings)!\")\n",
    "    \n",
    "    return trace, dynamic_ar_model, rho_block_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a80bc2-f12e-45b1-bb36-7ebd68cd5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    nrmse = rmse / (np.max(y_true) - np.min(y_true))\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'nrmse': nrmse\n",
    "    }\n",
    "\n",
    "def robust_smooth_acceleration(velocity, dt=0.5, window_size=7, poly_order=2):\n",
    "    if len(velocity) < window_size:\n",
    "        acceleration = np.gradient(velocity, dt)\n",
    "        return acceleration\n",
    "    \n",
    "    try:\n",
    "        acceleration = signal.savitzky_golay(velocity, window_length=window_size, \n",
    "                                           polyorder=poly_order, deriv=1, delta=dt)\n",
    "        \n",
    "        if len(acceleration) > 10:\n",
    "            x_fit = np.arange(5) * dt\n",
    "            y_fit = acceleration[5:10]\n",
    "            if len(y_fit) >= 2:\n",
    "                slope, intercept = np.polyfit(x_fit[:len(y_fit)], y_fit, 1)\n",
    "                for i in range(5):\n",
    "                    acceleration[i] = intercept + slope * (i * dt)\n",
    "            \n",
    "            x_fit = np.arange(5) * dt\n",
    "            y_fit = acceleration[-10:-5]\n",
    "            if len(y_fit) >= 2:\n",
    "                slope, intercept = np.polyfit(x_fit[:len(y_fit)], y_fit, 1)\n",
    "                for i in range(5):\n",
    "                    acceleration[-(5-i)] = intercept + slope * ((4-i) * dt)\n",
    "        \n",
    "        return acceleration\n",
    "        \n",
    "    except:\n",
    "        acceleration = np.zeros_like(velocity)\n",
    "        for i in range(1, len(velocity)-1):\n",
    "            acceleration[i] = (velocity[i+1] - velocity[i-1]) / (2 * dt)\n",
    "        \n",
    "        if len(velocity) > 1:\n",
    "            acceleration[0] = (velocity[1] - velocity[0]) / dt\n",
    "            acceleration[-1] = (velocity[-1] - velocity[-2]) / dt\n",
    "        \n",
    "        window = min(5, len(acceleration))\n",
    "        acceleration = np.convolve(acceleration, np.ones(window)/window, mode='same')\n",
    "        \n",
    "        return acceleration\n",
    "\n",
    "def calculate_initial_acceleration(velocity, dt=0.5, method='savitzky_golay'):\n",
    "    if len(velocity) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    if method == 'savitzky_golay':\n",
    "        acc_all = robust_smooth_acceleration(velocity, dt)\n",
    "        return acc_all[0]\n",
    "    \n",
    "    elif method == 'robust_fit':\n",
    "        n_points = min(5, len(velocity))\n",
    "        t_points = np.arange(n_points) * dt\n",
    "        v_points = velocity[:n_points]\n",
    "        \n",
    "        slope, intercept = np.polyfit(t_points, v_points, 1)\n",
    "        return slope\n",
    "    \n",
    "    elif method == 'physical_constrained':\n",
    "        if len(velocity) >= 4:\n",
    "            weights = np.array([0.1, 0.2, 0.3, 0.4])[:len(velocity)]\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            t_points = np.arange(len(velocity)) * dt\n",
    "            A = np.vstack([t_points, np.ones(len(t_points))]).T\n",
    "            W = np.diag(weights)\n",
    "            slope, intercept = np.linalg.lstsq(A.T @ W @ A, A.T @ W @ velocity, rcond=None)[0]\n",
    "            return slope\n",
    "        else:\n",
    "            return (velocity[1] - velocity[0]) / dt\n",
    "    \n",
    "    else:\n",
    "        initial_acc = (velocity[1] - velocity[0]) / dt\n",
    "        return np.clip(initial_acc, -3.0, 3.0)\n",
    "\n",
    "def improved_robust_acceleration(velocity, dt=0.5, window_size=7, poly_order=2):\n",
    "    acceleration = robust_smooth_acceleration(velocity, dt, window_size, poly_order)\n",
    "    \n",
    "    acceleration = np.clip(acceleration, -3.0, 3.0)\n",
    "    \n",
    "    if len(acceleration) > 10:\n",
    "        acceleration[:3] = np.mean(acceleration[:5])\n",
    "        acceleration[-3:] = np.mean(acceleration[-5:])\n",
    "    \n",
    "    return acceleration\n",
    "\n",
    "def split_data_for_ar_idm(ar_idm_data, train_ratio=0.7):\n",
    "    vt = ar_idm_data['vt']\n",
    "    s = ar_idm_data['s']\n",
    "    dv = ar_idm_data['dv']\n",
    "    label_v = ar_idm_data['label_v']\n",
    "    id_idx = ar_idm_data['id_idx']\n",
    "    \n",
    "    unique_vehicles = np.unique(id_idx)\n",
    "    \n",
    "    train_data = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    val_data = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    val_data1 = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        mask = (id_idx == veh_id)\n",
    "        n_points = np.sum(mask)\n",
    "        \n",
    "        if n_points < 20:\n",
    "            continue\n",
    "            \n",
    "        split_point = int(n_points * train_ratio)\n",
    "        \n",
    "        train_mask = np.zeros_like(mask, dtype=bool)\n",
    "        train_indices = np.where(mask)[0][:split_point]\n",
    "        train_mask[train_indices] = True\n",
    "        \n",
    "        val_mask = np.zeros_like(mask, dtype=bool)\n",
    "        val_indices = np.where(mask)[0][split_point:]\n",
    "        val_mask[val_indices] = True\n",
    "        \n",
    "        val1_mask = np.zeros_like(mask, dtype=bool)\n",
    "        val1_indices = np.where(mask)[0][split_point:split_point + 100]\n",
    "        val1_mask[val1_indices] = True\n",
    "        \n",
    "        train_data['vt'] = np.concatenate([train_data['vt'], vt[train_mask]])\n",
    "        train_data['s'] = np.concatenate([train_data['s'], s[train_mask]])\n",
    "        train_data['dv'] = np.concatenate([train_data['dv'], dv[train_mask]])\n",
    "        train_data['label_v'] = np.concatenate([train_data['label_v'], label_v[train_mask]])\n",
    "        train_data['id_idx'] = np.concatenate([train_data['id_idx'], np.full(np.sum(train_mask), veh_id)])\n",
    "        \n",
    "        if np.sum(train_mask) > 0:\n",
    "            train_data['tracks'][veh_id] = {\n",
    "                'last_vt': vt[train_mask][-1],\n",
    "                'last_s': s[train_mask][-1],\n",
    "                'last_dv': dv[train_mask][-1] if len(dv[train_mask]) > 0 else 0.0,\n",
    "                'last_rho_blocks': {}\n",
    "            }\n",
    "        \n",
    "        val_data['vt'] = np.concatenate([val_data['vt'], vt[val_mask]])\n",
    "        val_data['s'] = np.concatenate([val_data['s'], s[val_mask]])\n",
    "        val_data['dv'] = np.concatenate([val_data['dv'], dv[val_mask]])\n",
    "        val_data['label_v'] = np.concatenate([val_data['label_v'], label_v[val_mask]])\n",
    "        val_data['id_idx'] = np.concatenate([val_data['id_idx'], np.full(np.sum(val_mask), veh_id)])\n",
    "        \n",
    "        if np.sum(val1_mask) > 0:\n",
    "            val_data1['vt'] = np.concatenate([val_data1['vt'], vt[val1_mask]])\n",
    "            val_data1['s'] = np.concatenate([val_data1['s'], s[val1_mask]])\n",
    "            val_data1['dv'] = np.concatenate([val_data1['dv'], dv[val1_mask]])\n",
    "            val_data1['label_v'] = np.concatenate([val_data1['label_v'], label_v[val1_mask]])\n",
    "            val_data1['id_idx'] = np.concatenate([val_data1['id_idx'], np.full(np.sum(val1_mask), veh_id)])\n",
    "    \n",
    "    print(f\"Training set: {len(train_data['vt'])} data points\")\n",
    "    print(f\"Full validation set: {len(val_data['vt'])} data points\")\n",
    "    print(f\"Val_data1 (first 30s): {len(val_data1['vt'])} data points\")\n",
    "    \n",
    "    return train_data, val_data, val_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140de37a-1eee-45ae-8659-9ccf856ce267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dynamic_ar_model_comprehensive(trace, model, train_data, val_data, rho_block_info, n_samples=50, d=1, step=1, tau_sec=5):\n",
    "    print(\"\\nStarting DYNAMIC AR model validation with posterior sampling...\")\n",
    "    \n",
    "    n_chains = len(trace.posterior.chain)\n",
    "    n_draws = len(trace.posterior.draw)\n",
    "    \n",
    "    sample_indices = []\n",
    "    for _ in range(n_samples):\n",
    "        chain_idx = np.random.randint(0, n_chains)\n",
    "        draw_idx = np.random.randint(0, n_draws)\n",
    "        sample_indices.append((chain_idx, draw_idx))\n",
    "    \n",
    "    vt_val = val_data['vt']\n",
    "    s_val = val_data['s'] \n",
    "    dv_val = val_data['dv']\n",
    "    label_val = val_data['label_v']\n",
    "    id_idx_val = val_data['id_idx']\n",
    "    \n",
    "    dt = 0.5\n",
    "    DELTA = 4\n",
    "    tau = int(tau_sec / dt)\n",
    "    \n",
    "    print(\"Calculating real acceleration using improved robust method...\")\n",
    "    real_acceleration_all = improved_robust_acceleration(vt_val, dt)\n",
    "    \n",
    "    all_samples_speed_predictions = []\n",
    "    all_samples_spacing_predictions = []\n",
    "    all_samples_acceleration_predictions = []\n",
    "    all_samples_ar_coefficients = []\n",
    "    all_samples_vehicle_predictions = {}\n",
    "    \n",
    "    print(\"Extracting numerical block information for all vehicles...\")\n",
    "    numerical_block_info = {}\n",
    "    \n",
    "    for veh_id in range(val_data['n_vehicles']):\n",
    "        train_mask = (train_data['id_idx'] == veh_id)\n",
    "        n_train_timesteps = int(np.sum(train_mask))\n",
    "        \n",
    "        if n_train_timesteps > (d * step) + 5:\n",
    "            n_train_blocks = int(np.ceil(n_train_timesteps / tau))\n",
    "            \n",
    "            block_values_numerical = []\n",
    "            for lag in range(d):\n",
    "                try:\n",
    "                    init_key = f'rho_init_{veh_id}_lag{lag}'\n",
    "                    if init_key in trace.posterior:\n",
    "                        init_val = trace.posterior[init_key].mean(dim=(\"chain\", \"draw\")).item()\n",
    "                    else:\n",
    "                        init_val = trace.posterior['rho_0'].mean(dim=(\"chain\", \"draw\")).sel(veh_id=veh_id, ar_lag=lag).item()\n",
    "                    \n",
    "                    innov_key = f'rho_innov_blocks_{veh_id}_lag{lag}'\n",
    "                    if innov_key in trace.posterior:\n",
    "                        innov_vals = trace.posterior[innov_key].mean(dim=(\"chain\", \"draw\")).values\n",
    "                    else:\n",
    "                        innov_vals = np.zeros(max(0, n_train_blocks - 1))\n",
    "                    \n",
    "                    block_vals = [init_val]\n",
    "                    for innov in innov_vals:\n",
    "                        block_vals.append(block_vals[-1] + innov)\n",
    "                    \n",
    "                    block_values_numerical.append(np.array(block_vals))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    default_val = trace.posterior['rho_0'].mean(dim=(\"chain\", \"draw\")).sel(veh_id=veh_id, ar_lag=lag).item()\n",
    "                    block_values_numerical.append(np.array([default_val]))\n",
    "            \n",
    "            numerical_block_info[veh_id] = {\n",
    "                'n_blocks': n_train_blocks,\n",
    "                'block_values': block_values_numerical,\n",
    "                'tau': tau,\n",
    "                'n_timesteps': n_train_timesteps\n",
    "            }\n",
    "        else:\n",
    "            numerical_block_info[veh_id] = None\n",
    "    \n",
    "    print(f\"Extracted numerical block information for {len([x for x in numerical_block_info.values() if x is not None])} vehicles\")\n",
    "    \n",
    "    for sample_idx, (chain_idx, draw_idx) in enumerate(sample_indices):\n",
    "        print(f\"Processing posterior sample {sample_idx + 1}/{n_samples}...\")\n",
    "        \n",
    "        individual_params = trace.posterior['parameters'].sel(chain=chain_idx, draw=draw_idx).values\n",
    "        rho_0 = trace.posterior['rho_0'].sel(chain=chain_idx, draw=draw_idx).values\n",
    "        rho_innovation = trace.posterior['rho_innovation'].sel(chain=chain_idx, draw=draw_idx).values\n",
    "        \n",
    "        rho_innov_blocks_dict = {}\n",
    "        rho_init_dict = {}\n",
    "        \n",
    "        for i in range(val_data['n_vehicles']):\n",
    "            for lag in range(d):\n",
    "                try:\n",
    "                    innov_key = f'rho_innov_blocks_{i}_lag{lag}'\n",
    "                    if innov_key in trace.posterior:\n",
    "                        rho_innov_blocks_dict[(i, lag)] = trace.posterior[innov_key].sel(\n",
    "                            chain=chain_idx, draw=draw_idx).values\n",
    "                    \n",
    "                    init_key = f'rho_init_{i}_lag{lag}'\n",
    "                    if init_key in trace.posterior:\n",
    "                        rho_init_dict[(i, lag)] = trace.posterior[init_key].sel(\n",
    "                            chain=chain_idx, draw=draw_idx).values\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        all_speed_predictions = []\n",
    "        all_spacing_predictions = []\n",
    "        all_acceleration_predictions = []\n",
    "        all_ar_coefficients_veh = []\n",
    "        sample_vehicle_predictions = {}\n",
    "        \n",
    "        unique_vehicles = np.unique(id_idx_val)\n",
    "        \n",
    "        for veh_id in unique_vehicles:\n",
    "            mask = (id_idx_val == veh_id)\n",
    "            if np.sum(mask) > d + 5:\n",
    "                vt_veh = vt_val[mask]\n",
    "                s_veh = s_val[mask]\n",
    "                dv_veh = dv_val[mask]\n",
    "                real_accel_veh = real_acceleration_all[mask]\n",
    "                \n",
    "                initial_v = vt_veh[0]\n",
    "                initial_s = s_veh[0]\n",
    "                initial_dv = dv_veh[0] if len(dv_veh) > 0 else 0.0\n",
    "                \n",
    "                if veh_id in train_data['tracks']:\n",
    "                    train_mask = (train_data['id_idx'] == veh_id)\n",
    "                    if np.sum(train_mask) >= 5:\n",
    "                        train_vt = train_data['vt'][train_mask]\n",
    "                        initial_acc = calculate_initial_acceleration(train_vt[-5:], dt, method='savitzky_golay')\n",
    "                    else:\n",
    "                        initial_acc = 0.0\n",
    "                else:\n",
    "                    initial_acc = 0.0\n",
    "                \n",
    "                vmax = 25 * individual_params[veh_id, 0]\n",
    "                dsafe = 2 * individual_params[veh_id,1]\n",
    "                tsafe = 1.6 * individual_params[veh_id, 2]\n",
    "                amax = 1.5 * individual_params[veh_id, 3]\n",
    "                amin = 1.5 * individual_params[veh_id, 4]\n",
    "                \n",
    "                initial_acc = np.clip(initial_acc, -amin, amax)\n",
    "                \n",
    "                warmup_steps = 5\n",
    "                \n",
    "                n_val_points = len(vt_veh)\n",
    "                n_val_blocks = int(np.ceil(n_val_points / tau))\n",
    "                \n",
    "                rho_dynamic_val = np.zeros((n_val_points, d))\n",
    "                \n",
    "                for lag in range(d):\n",
    "                    rho_0_val = rho_0[veh_id, lag]\n",
    "                    \n",
    "                    if veh_id in numerical_block_info and numerical_block_info[veh_id] is not None:\n",
    "                        train_block_values = numerical_block_info[veh_id]['block_values'][lag]\n",
    "                        if len(train_block_values) > 0:\n",
    "                            last_train_block_value = train_block_values[-1]\n",
    "                        else:\n",
    "                            last_train_block_value = rho_0_val\n",
    "                    else:\n",
    "                        if (veh_id, lag) in rho_init_dict:\n",
    "                            last_train_block_value = rho_init_dict[(veh_id, lag)]\n",
    "                        else:\n",
    "                            last_train_block_value = rho_0_val\n",
    "                    \n",
    "                    if n_val_blocks > 0:\n",
    "                        if (veh_id, lag) in rho_innov_blocks_dict:\n",
    "                            innov_available = rho_innov_blocks_dict[(veh_id, lag)]\n",
    "                            n_innov_needed = n_val_blocks\n",
    "                            if len(innov_available) >= n_innov_needed:\n",
    "                                val_innov = innov_available[:n_innov_needed]\n",
    "                            else:\n",
    "                                n_additional = n_innov_needed - len(innov_available)\n",
    "                                additional_innov = np.random.normal(0, rho_innovation[veh_id, lag], n_additional)\n",
    "                                val_innov = np.concatenate([innov_available, additional_innov])\n",
    "                        else:\n",
    "                            val_innov = np.random.normal(0, rho_innovation[veh_id, lag], n_val_blocks)\n",
    "                        \n",
    "                        val_block_values = [last_train_block_value]\n",
    "                        for i in range(n_val_blocks):\n",
    "                            new_value = val_block_values[-1] + val_innov[i]\n",
    "                            val_block_values.append(new_value)\n",
    "                        \n",
    "                        val_block_values = val_block_values[1:]\n",
    "                        \n",
    "                        for block_idx, block_value in enumerate(val_block_values):\n",
    "                            start_idx = block_idx * tau\n",
    "                            end_idx = min((block_idx + 1) * tau, n_val_points)\n",
    "                            rho_dynamic_val[start_idx:end_idx, lag] = block_value\n",
    "                    else:\n",
    "                        rho_dynamic_val[:, lag] = last_train_block_value\n",
    "                \n",
    "                all_ar_coefficients_veh.append(rho_dynamic_val)\n",
    "                \n",
    "                v_sim = [initial_v]\n",
    "                s_sim = [initial_s]\n",
    "                a_sim = [initial_acc]\n",
    "                \n",
    "                v_history_real = deque([initial_v], maxlen=d+1)\n",
    "                idm_acc_history = deque([initial_acc], maxlen=d)\n",
    "                acceleration_errors = deque([], maxlen=d)\n",
    "                \n",
    "                n = len(vt_veh)\n",
    "                \n",
    "                for i in range(n-1):\n",
    "                    if i < warmup_steps and i < len(vt_veh) - 1:\n",
    "                        v_next_real = vt_veh[i + 1] if (i + 1) < len(vt_veh) else vt_veh[i]\n",
    "                        s_next_real = s_veh[i + 1] if (i + 1) < len(s_veh) else s_veh[i]\n",
    "                        a_next_real = real_accel_veh[i] if i < len(real_accel_veh) else 0.0\n",
    "                        \n",
    "                        s_next_real = max(1.0, min(200.0, s_next_real))\n",
    "                        a_next_real = np.clip(a_next_real, -amin, amax)\n",
    "                        \n",
    "                        v_sim.append(v_next_real)\n",
    "                        s_sim.append(s_next_real)\n",
    "                        a_sim.append(a_next_real)\n",
    "                        \n",
    "                        v_history_real.append(v_next_real)\n",
    "                        \n",
    "                        current_dv = dv_veh[i] if i < len(dv_veh) else 0.0\n",
    "                        sn = dsafe + v_sim[-2] * tsafe + v_sim[-2] * current_dv / (2 * np.sqrt(amax * amin))\n",
    "                        current_acc_idm = amax * (1 - (v_sim[-2] / vmax) ** DELTA - (sn / s_sim[-2]) ** 2)\n",
    "                        current_acc_idm = np.clip(current_acc_idm, -amin, amax)\n",
    "                        idm_acc_history.append(current_acc_idm)\n",
    "                        \n",
    "                        if i > 0:\n",
    "                            a_real_prev = real_accel_veh[i-1] if (i-1) < len(real_accel_veh) else 0.0\n",
    "                            prev_dv = dv_veh[i-1] if (i-1) < len(dv_veh) else 0.0\n",
    "                            prev_sn = dsafe + v_sim[-3] * tsafe + v_sim[-3] * prev_dv / (2 * np.sqrt(amax * amin))\n",
    "                            a_idm_prev = amax * (1 - (v_sim[-3] / vmax) ** DELTA - (prev_sn / s_sim[-3]) ** 2)\n",
    "                            a_idm_prev = np.clip(a_idm_prev, -amin, amax)\n",
    "                            a_error = a_real_prev - a_idm_prev\n",
    "                            acceleration_errors.append(a_error)\n",
    "                    else:\n",
    "                        if i > 0:\n",
    "                            a_real_prev = real_accel_veh[i-1] if (i-1) < len(real_accel_veh) else 0.0\n",
    "                            prev_dv = dv_veh[i-1] if (i-1) < len(dv_veh) else 0.0\n",
    "                            prev_sn = dsafe + v_sim[-2] * tsafe + v_sim[-2] * prev_dv / (2 * np.sqrt(amax * amin))\n",
    "                            a_idm_prev = amax * (1 - (v_sim[-2] / vmax) ** DELTA - (prev_sn / s_sim[-2]) ** 2)\n",
    "                            a_idm_prev = np.clip(a_idm_prev, -amin, amax)\n",
    "                            a_error = a_real_prev - a_idm_prev\n",
    "                            acceleration_errors.append(a_error)\n",
    "                        \n",
    "                        current_dv = dv_veh[i] if i < len(dv_veh) else 0.0\n",
    "                        sn = dsafe + v_sim[-1] * tsafe + v_sim[-1] * current_dv / (2 * np.sqrt(amax * amin))\n",
    "                        current_acc_idm = amax * (1 - (v_sim[-1] / vmax) ** DELTA - (sn / s_sim[-1]) ** 2)\n",
    "                        current_acc_idm = np.clip(current_acc_idm, -amin, amax)\n",
    "                        \n",
    "                        v_next_base = v_sim[-1] + current_acc_idm * dt\n",
    "                        \n",
    "                        v_ar_correction = 0.0\n",
    "                        if len(v_history_real) > d:\n",
    "                            for lag in range(d):\n",
    "                                if len(v_history_real) > lag + 1:\n",
    "                                    vt_diff = v_history_real[-1] - v_history_real[-(lag+2)]\n",
    "                                    if len(idm_acc_history) > lag:\n",
    "                                        a_lag = idm_acc_history[-(lag+1)]\n",
    "                                    else:\n",
    "                                        a_lag = 0.0\n",
    "                                    \n",
    "                                    ar_coef = rho_dynamic_val[i, lag]\n",
    "                                    v_ar_correction += ar_coef * (vt_diff - a_lag * dt)\n",
    "                        \n",
    "                        v_next_after_v_correction = v_next_base + v_ar_correction\n",
    "                        \n",
    "                        a_ar_correction = 0.0\n",
    "                        if len(acceleration_errors) >= d:\n",
    "                            for lag in range(d):\n",
    "                                if len(acceleration_errors) > lag:\n",
    "                                    a_ar_correction += rho_dynamic_val[i, lag] * acceleration_errors[-(lag+1)]\n",
    "                        \n",
    "                        current_acc_total = current_acc_idm + a_ar_correction\n",
    "                        current_acc_total = np.clip(current_acc_total, -amin, amax)\n",
    "                        \n",
    "                        s_next = s_sim[-1] + (current_dv +0.5*v_ar_correction)* dt+0.5*dt*current_acc_idm * dt\n",
    "                        s_next = max(1.0, min(200.0, s_next))\n",
    "                        \n",
    "                        v_sim.append(v_next_after_v_correction)\n",
    "                        s_sim.append(s_next)\n",
    "                        a_sim.append(current_acc_total)\n",
    "                        \n",
    "                        if i + 1 < len(vt_veh):\n",
    "                            v_history_real.append(vt_veh[i + 1])\n",
    "                        else:\n",
    "                            v_history_real.append(v_next_after_v_correction)\n",
    "                        \n",
    "                        idm_acc_history.append(current_acc_idm)\n",
    "                \n",
    "                ar_corrected_speed = np.array(v_sim)\n",
    "                corrected_spacing = np.array(s_sim)\n",
    "                corrected_acceleration = np.array(a_sim)\n",
    "                \n",
    "                all_speed_predictions.extend(ar_corrected_speed[:n])\n",
    "                all_spacing_predictions.extend(corrected_spacing[:n])\n",
    "                all_acceleration_predictions.extend(corrected_acceleration[:n])\n",
    "                \n",
    "                sample_vehicle_predictions[veh_id] = {\n",
    "                    'speed_pred': ar_corrected_speed[:n],\n",
    "                    'spacing_pred': corrected_spacing[:n],\n",
    "                    'acceleration_pred': corrected_acceleration[:n],\n",
    "                    'speed_true': vt_veh,\n",
    "                    'spacing_true': s_veh,\n",
    "                    'acceleration_true': real_accel_veh,\n",
    "                    'ar_coefficients': rho_dynamic_val,\n",
    "                    'warmup_steps': warmup_steps\n",
    "                }\n",
    "                \n",
    "            else:\n",
    "                if veh_id in train_data['tracks']:\n",
    "                    initial_v = train_data['tracks'][veh_id]['last_vt']\n",
    "                    initial_s = train_data['tracks'][veh_id]['last_s']\n",
    "                else:\n",
    "                    initial_v = vt_veh[0] if len(vt_veh) > 0 else 0.0\n",
    "                    initial_s = s_veh[0] if len(s_veh) > 0 else 10.0\n",
    "                \n",
    "                simple_speed = np.full(len(vt_veh), initial_v)\n",
    "                simple_spacing = np.full(len(s_veh), initial_s)\n",
    "                simple_acceleration = np.zeros(len(vt_veh))\n",
    "                \n",
    "                all_speed_predictions.extend(simple_speed)\n",
    "                all_spacing_predictions.extend(simple_spacing)\n",
    "                all_acceleration_predictions.extend(simple_acceleration)\n",
    "                \n",
    "                empty_ar_coeffs = np.zeros((len(vt_veh), d))\n",
    "                all_ar_coefficients_veh.append(empty_ar_coeffs)\n",
    "                \n",
    "                sample_vehicle_predictions[veh_id] = {\n",
    "                    'speed_pred': simple_speed,\n",
    "                    'spacing_pred': simple_spacing,\n",
    "                    'acceleration_pred': simple_acceleration,\n",
    "                    'speed_true': vt_veh,\n",
    "                    'spacing_true': s_veh,\n",
    "                    'acceleration_true': real_accel_veh if len(real_accel_veh) == len(simple_speed) else np.zeros(len(simple_speed)),\n",
    "                    'ar_coefficients': empty_ar_coeffs,\n",
    "                    'warmup_steps': 0\n",
    "                }\n",
    "        \n",
    "        all_speed_predictions = np.array(all_speed_predictions)\n",
    "        all_spacing_predictions = np.array(all_spacing_predictions)\n",
    "        all_acceleration_predictions = np.array(all_acceleration_predictions)\n",
    "        \n",
    "        all_samples_speed_predictions.append(all_speed_predictions)\n",
    "        all_samples_spacing_predictions.append(all_spacing_predictions)\n",
    "        all_samples_acceleration_predictions.append(all_acceleration_predictions)\n",
    "        all_samples_ar_coefficients.append(all_ar_coefficients_veh)\n",
    "        all_samples_vehicle_predictions[sample_idx] = sample_vehicle_predictions\n",
    "    \n",
    "    min_len = min(\n",
    "        len(all_samples_speed_predictions[0]), \n",
    "        len(all_samples_spacing_predictions[0]),\n",
    "        len(all_samples_acceleration_predictions[0]), \n",
    "        len(real_acceleration_all),\n",
    "        len(label_val), \n",
    "        len(s_val)\n",
    "    )\n",
    "    \n",
    "    boundary_cut = int(0.05 * min_len)\n",
    "    valid_indices = slice(boundary_cut, min_len - boundary_cut)\n",
    "    \n",
    "    print(f\"Excluding boundary segments: using indices {boundary_cut} to {min_len - boundary_cut}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-VEHICLE VALIDATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    vehicle_metrics = {}\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        veh_speed_preds = []\n",
    "        veh_spacing_preds = []\n",
    "        veh_accel_preds = []\n",
    "        veh_speed_true = None\n",
    "        veh_spacing_true = None\n",
    "        veh_accel_true = None\n",
    "        \n",
    "        for sample_idx in range(n_samples):\n",
    "            if veh_id in all_samples_vehicle_predictions[sample_idx]:\n",
    "                veh_data = all_samples_vehicle_predictions[sample_idx][veh_id]\n",
    "                veh_speed_preds.append(veh_data['speed_pred'])\n",
    "                veh_spacing_preds.append(veh_data['spacing_pred'])\n",
    "                veh_accel_preds.append(veh_data['acceleration_pred'])\n",
    "                \n",
    "                if veh_speed_true is None:\n",
    "                    veh_speed_true = veh_data['speed_true']\n",
    "                    veh_spacing_true = veh_data['spacing_true']\n",
    "                    veh_accel_true = veh_data['acceleration_true']\n",
    "        \n",
    "        if veh_speed_true is not None and len(veh_speed_true) > 10:\n",
    "            avg_speed_pred = np.mean(veh_speed_preds, axis=0)\n",
    "            avg_spacing_pred = np.mean(veh_spacing_preds, axis=0)\n",
    "            avg_accel_pred = np.mean(veh_accel_preds, axis=0)\n",
    "            \n",
    "            veh_boundary_cut = int(0.05 * len(veh_speed_true))\n",
    "            veh_valid_indices = slice(veh_boundary_cut, len(veh_speed_true) - veh_boundary_cut)\n",
    "            \n",
    "            speed_metrics = calculate_metrics(\n",
    "                veh_speed_true[veh_valid_indices], \n",
    "                avg_speed_pred[veh_valid_indices]\n",
    "            )\n",
    "            spacing_metrics = calculate_metrics(\n",
    "                veh_spacing_true[veh_valid_indices], \n",
    "                avg_spacing_pred[veh_valid_indices]\n",
    "            )\n",
    "            acceleration_metrics = calculate_metrics(\n",
    "                veh_accel_true[veh_valid_indices],\n",
    "                avg_accel_pred[veh_valid_indices]\n",
    "            )\n",
    "            \n",
    "            vehicle_metrics[veh_id] = {\n",
    "                'speed': speed_metrics,\n",
    "                'spacing': spacing_metrics,\n",
    "                'acceleration': acceleration_metrics,\n",
    "                'n_points': len(veh_speed_true[veh_valid_indices])\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nVehicle {veh_id} (n={vehicle_metrics[veh_id]['n_points']}):\")\n",
    "            print(f\"  Speed - RMSE: {speed_metrics['rmse']:.4f} m/s, MAE: {speed_metrics['mae']:.4f} m/s, NRMSE: {speed_metrics['nrmse']:.4f}\")\n",
    "            print(f\"  Spacing - RMSE: {spacing_metrics['rmse']:.4f} m, MAE: {spacing_metrics['mae']:.4f} m, NRMSE: {spacing_metrics['nrmse']:.4f}\")\n",
    "            print(f\"  Acceleration - RMSE: {acceleration_metrics['rmse']:.4f} m/s², MAE: {acceleration_metrics['mae']:.4f} m/s², NRMSE: {acceleration_metrics['nrmse']:.4f}\")\n",
    "    \n",
    "    speed_metrics_all = []\n",
    "    spacing_metrics_all = []\n",
    "    acceleration_metrics_all = []\n",
    "    \n",
    "    for i in range(n_samples): \n",
    "        speed_metrics = calculate_metrics(\n",
    "            label_val[valid_indices], \n",
    "            all_samples_speed_predictions[i][valid_indices]\n",
    "        )\n",
    "        spacing_metrics = calculate_metrics(\n",
    "            s_val[valid_indices], \n",
    "            all_samples_spacing_predictions[i][valid_indices]\n",
    "        )\n",
    "        acceleration_metrics = calculate_metrics(\n",
    "            real_acceleration_all[valid_indices],\n",
    "            all_samples_acceleration_predictions[i][valid_indices]\n",
    "        )\n",
    "        \n",
    "        speed_metrics_all.append(speed_metrics)\n",
    "        spacing_metrics_all.append(spacing_metrics)\n",
    "        acceleration_metrics_all.append(acceleration_metrics)\n",
    "    \n",
    "    avg_speed_metrics = {\n",
    "        'mse': np.mean([m['mse'] for m in speed_metrics_all]),\n",
    "        'rmse': np.mean([m['rmse'] for m in speed_metrics_all]),\n",
    "        'mae': np.mean([m['mae'] for m in speed_metrics_all]),\n",
    "        'nrmse': np.mean([m['nrmse'] for m in speed_metrics_all])\n",
    "    }\n",
    "    \n",
    "    avg_spacing_metrics = {\n",
    "        'mse': np.mean([m['mse'] for m in spacing_metrics_all]),\n",
    "        'rmse': np.mean([m['rmse'] for m in spacing_metrics_all]),\n",
    "        'mae': np.mean([m['mae'] for m in spacing_metrics_all]),\n",
    "        'nrmse': np.mean([m['nrmse'] for m in spacing_metrics_all])\n",
    "    }\n",
    "    \n",
    "    avg_acceleration_metrics = {\n",
    "        'mse': np.mean([m['mse'] for m in acceleration_metrics_all]),\n",
    "        'rmse': np.mean([m['rmse'] for m in acceleration_metrics_all]),\n",
    "        'mae': np.mean([m['mae'] for m in acceleration_metrics_all]),\n",
    "        'nrmse': np.mean([m['nrmse'] for m in acceleration_metrics_all])\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VEHICLE PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if vehicle_metrics:\n",
    "        speed_rmse_values = [vm['speed']['rmse'] for vm in vehicle_metrics.values()]\n",
    "        spacing_rmse_values = [vm['spacing']['rmse'] for vm in vehicle_metrics.values()]\n",
    "        accel_rmse_values = [vm['acceleration']['rmse'] for vm in vehicle_metrics.values()]\n",
    "        \n",
    "        print(f\"Number of vehicles analyzed: {len(vehicle_metrics)}\")\n",
    "        print(f\"\\nSpeed RMSE - Mean: {np.mean(speed_rmse_values):.4f}, Std: {np.std(speed_rmse_values):.4f}, \"\n",
    "              f\"Min: {np.min(speed_rmse_values):.4f}, Max: {np.max(speed_rmse_values):.4f}\")\n",
    "        print(f\"Spacing RMSE - Mean: {np.mean(spacing_rmse_values):.4f}, Std: {np.std(spacing_rmse_values):.4f}, \"\n",
    "              f\"Min: {np.min(spacing_rmse_values):.4f}, Max: {np.max(spacing_rmse_values):.4f}\")\n",
    "        print(f\"Acceleration RMSE - Mean: {np.mean(accel_rmse_values):.4f}, Std: {np.std(accel_rmse_values):.4f}, \"\n",
    "              f\"Min: {np.min(accel_rmse_values):.4f}, Max: {np.max(accel_rmse_values):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OVERALL VALIDATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Dynamic AR Validation Results (using {n_samples} posterior samples, excluding boundaries):\")\n",
    "    print(f\"\\nSpeed Metrics:\")\n",
    "    print(f\"  RMSE: {avg_speed_metrics['rmse']:.4f} m/s\")\n",
    "    print(f\"  NRMSE: {avg_speed_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MAE: {avg_speed_metrics['mae']:.4f} m/s\")\n",
    "    \n",
    "    print(f\"\\nSpacing Metrics:\")\n",
    "    print(f\"  RMSE: {avg_spacing_metrics['rmse']:.4f} m\")\n",
    "    print(f\"  NRMSE: {avg_spacing_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MAE: {avg_spacing_metrics['mae']:.4f} m\")\n",
    "    \n",
    "    print(f\"\\nAcceleration Metrics:\")\n",
    "    print(f\"  RMSE: {avg_acceleration_metrics['rmse']:.4f} m/s²\")\n",
    "    print(f\"  NRMSE: {avg_acceleration_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MAE: {avg_acceleration_metrics['mae']:.4f} m/s²\")\n",
    "    \n",
    "    return {\n",
    "        'speed_metrics': avg_speed_metrics,\n",
    "        'spacing_metrics': avg_spacing_metrics,\n",
    "        'acceleration_metrics': avg_acceleration_metrics,\n",
    "        'vehicle_metrics': vehicle_metrics,\n",
    "        'all_samples_speed_predictions': all_samples_speed_predictions,\n",
    "        'all_samples_spacing_predictions': all_samples_spacing_predictions,\n",
    "        'all_samples_acceleration_predictions': all_samples_acceleration_predictions,\n",
    "        'all_samples_ar_coefficients': all_samples_ar_coefficients,\n",
    "        'all_samples_vehicle_predictions': all_samples_vehicle_predictions,\n",
    "        'real_acceleration': real_acceleration_all,\n",
    "        'individual_params': trace.posterior['parameters'].mean(dim=(\"chain\", \"draw\")).values,\n",
    "        'ar_coeffs': trace.posterior['rho_0'].mean(dim=(\"chain\", \"draw\")).values,\n",
    "        'valid_indices': valid_indices,\n",
    "        'n_samples': n_samples,\n",
    "        'rho_block_info': numerical_block_info,\n",
    "        'warmup_steps': warmup_steps\n",
    "    }\n",
    "\n",
    "def plot_dynamic_ar_validation_results(val_data, validation_results, tau_sec=3):\n",
    "    vt_val = val_data['vt']\n",
    "    s_val = val_data['s']\n",
    "    label_val = val_data['label_v']\n",
    "    id_idx_val = val_data['id_idx']\n",
    "    \n",
    "    all_samples_speed = validation_results['all_samples_speed_predictions']\n",
    "    all_samples_spacing = validation_results['all_samples_spacing_predictions']\n",
    "    all_samples_acceleration = validation_results['all_samples_acceleration_predictions']\n",
    "    all_samples_ar_coefficients = validation_results['all_samples_ar_coefficients']\n",
    "    real_acceleration = validation_results['real_acceleration']\n",
    "    valid_indices = validation_results['valid_indices']\n",
    "    n_samples = validation_results['n_samples']\n",
    "    \n",
    "    unique_vehicles = np.unique(id_idx_val)\n",
    "    n_vehicles = len(unique_vehicles)\n",
    "    \n",
    "    dt = 0.5\n",
    "    tau = int(tau_sec / dt)\n",
    "    \n",
    "    title_fontsize = 14\n",
    "    label_fontsize = 12\n",
    "    legend_fontsize = 11\n",
    "    tick_fontsize = 10\n",
    "    \n",
    "    for veh_idx, veh_id in enumerate(unique_vehicles):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        mask = (id_idx_val == veh_id)\n",
    "        if np.sum(mask) > 0:\n",
    "            veh_data_points = min(np.sum(mask), len(all_samples_speed[0]))\n",
    "            time_points = np.arange(veh_data_points) * dt\n",
    "            \n",
    "            axes[0, 0].plot(time_points, label_val[mask][:veh_data_points], 'k-', \n",
    "                           linewidth=3, alpha=0.9)\n",
    "            \n",
    "            veh_speed_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_speed])\n",
    "            \n",
    "            lower_5 = np.percentile(veh_speed_samples, 5, axis=0)\n",
    "            upper_95 = np.percentile(veh_speed_samples, 95, axis=0)\n",
    "            lower_25 = np.percentile(veh_speed_samples, 25, axis=0)\n",
    "            upper_75 = np.percentile(veh_speed_samples, 75, axis=0)\n",
    "            \n",
    "            axes[0, 0].fill_between(time_points, lower_5, upper_95, \n",
    "                                   alpha=0.3, color='red')\n",
    "            axes[0, 0].fill_between(time_points, lower_25, upper_75, \n",
    "                                   alpha=0.5, color='red', label=f'Driver {veh_id+1}')\n",
    "            \n",
    "            mean_speed = np.mean(veh_speed_samples, axis=0)\n",
    "            axes[0, 0].plot(time_points, mean_speed, 'b-', \n",
    "                           linewidth=2, alpha=0.8)\n",
    "            \n",
    "            axes[0, 0].set_xlabel('Time (s)', fontsize=label_fontsize)\n",
    "            axes[0, 0].set_ylabel('Speed (m/s)', fontsize=label_fontsize)\n",
    "            axes[0, 0].legend(fontsize=legend_fontsize)\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "            \n",
    "            axes[0, 1].plot(time_points, s_val[mask][:veh_data_points], 'k-', \n",
    "                           linewidth=3, alpha=0.9)\n",
    "            \n",
    "            veh_spacing_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_spacing])\n",
    "            \n",
    "            lower_5_s = np.percentile(veh_spacing_samples, 5, axis=0)\n",
    "            upper_95_s = np.percentile(veh_spacing_samples, 95, axis=0)\n",
    "            \n",
    "            axes[0, 1].fill_between(time_points, lower_5_s, upper_95_s, \n",
    "                                   alpha=0.3, color='green', label=f'Driver {veh_id+1}')\n",
    "            \n",
    "            mean_spacing = np.mean(veh_spacing_samples, axis=0)\n",
    "            axes[0, 1].plot(time_points, mean_spacing, 'g-', \n",
    "                           linewidth=2, alpha=0.8)\n",
    "            \n",
    "            axes[0, 1].set_xlabel('Time (s)', fontsize=label_fontsize)\n",
    "            axes[0, 1].set_ylabel('Spacing (m)', fontsize=label_fontsize)\n",
    "            axes[0, 1].legend(fontsize=legend_fontsize)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            axes[0, 1].tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "            \n",
    "            axes[0, 2].plot(time_points, real_acceleration[mask][:veh_data_points], 'k-', \n",
    "                           linewidth=3, alpha=0.9)\n",
    "            \n",
    "            veh_accel_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_acceleration])\n",
    "            \n",
    "            lower_5_a = np.percentile(veh_accel_samples, 5, axis=0)\n",
    "            upper_95_a = np.percentile(veh_accel_samples, 95, axis=0)\n",
    "            \n",
    "            axes[0, 2].fill_between(time_points, lower_5_a, upper_95_a, \n",
    "                                   alpha=0.3, color='orange', label=f'Driver {veh_id+1}')\n",
    "            \n",
    "            mean_acceleration = np.mean(veh_accel_samples, axis=0)\n",
    "            axes[0, 2].plot(time_points, mean_acceleration, 'c-', \n",
    "                           linewidth=2, alpha=0.8)\n",
    "            \n",
    "            axes[0, 2].set_xlabel('Time (s)', fontsize=label_fontsize)\n",
    "            axes[0, 2].set_ylabel('Acceleration (m/s²)', fontsize=label_fontsize)\n",
    "            axes[0, 2].legend(fontsize=legend_fontsize)\n",
    "            axes[0, 2].grid(True, alpha=0.3)\n",
    "            axes[0, 2].tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "            \n",
    "            if len(all_samples_ar_coefficients) > 0 and veh_idx < len(all_samples_ar_coefficients[0]):\n",
    "                sample_ar_coeffs = all_samples_ar_coefficients[0][veh_idx]\n",
    "                if len(sample_ar_coeffs) >= veh_data_points:\n",
    "                    for lag in range(min(4, sample_ar_coeffs.shape[1])):\n",
    "                        axes[1, 0].plot(time_points, sample_ar_coeffs[:veh_data_points, lag], \n",
    "                                       alpha=0.8)\n",
    "                \n",
    "                axes[1, 0].plot([], [], color='blue', label=f'Driver {veh_id+1}')\n",
    "                \n",
    "                axes[1, 0].set_xlabel('Time (s)', fontsize=label_fontsize)\n",
    "                axes[1, 0].set_ylabel('AR Coefficient Value', fontsize=label_fontsize)\n",
    "                axes[1, 0].legend(fontsize=legend_fontsize)\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "                axes[1, 0].tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "                \n",
    "                for block_idx in range(0, veh_data_points, tau):\n",
    "                    if block_idx < veh_data_points:\n",
    "                        axes[1, 0].axvline(x=block_idx*dt, color='gray', linestyle='--', alpha=0.5)\n",
    "            \n",
    "            if len(all_samples_ar_coefficients) > 0 and veh_idx < len(all_samples_ar_coefficients[0]):\n",
    "                all_veh_ar_samples = []\n",
    "                for sample_idx in range(min(10, len(all_samples_ar_coefficients))):\n",
    "                    if veh_idx < len(all_samples_ar_coefficients[sample_idx]):\n",
    "                        ar_coeffs = all_samples_ar_coefficients[sample_idx][veh_idx]\n",
    "                        if len(ar_coeffs) >= veh_data_points:\n",
    "                            all_veh_ar_samples.append(ar_coeffs[:veh_data_points, 0])\n",
    "                \n",
    "                if len(all_veh_ar_samples) > 0:\n",
    "                    all_veh_ar_samples = np.array(all_veh_ar_samples)\n",
    "                    lower_5_ar = np.percentile(all_veh_ar_samples, 5, axis=0)\n",
    "                    upper_95_ar = np.percentile(all_veh_ar_samples, 95, axis=0)\n",
    "                    mean_ar = np.mean(all_veh_ar_samples, axis=0)\n",
    "                    \n",
    "                    axes[1, 1].fill_between(time_points, lower_5_ar, upper_95_ar, \n",
    "                                          alpha=0.3, color='purple', label=f'Driver {veh_id+1}')\n",
    "                    axes[1, 1].plot(time_points, mean_ar, 'purple', \n",
    "                                   linewidth=2)\n",
    "                    \n",
    "                    axes[1, 1].set_xlabel('Time (s)', fontsize=label_fontsize)\n",
    "                    axes[1, 1].set_ylabel('AR Coefficient Value', fontsize=label_fontsize)\n",
    "                    axes[1, 1].legend(fontsize=legend_fontsize)\n",
    "                    axes[1, 1].grid(True, alpha=0.3)\n",
    "                    axes[1, 1].tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "            \n",
    "            speed_errors = label_val[mask][:veh_data_points] - mean_speed\n",
    "            axes[1, 2].plot(time_points, speed_errors, 'r-', alpha=0.7, label=f'Driver {veh_id+1}')\n",
    "            axes[1, 2].axhline(y=0, color='k', linestyle='-', alpha=0.5)\n",
    "            axes[1, 2].fill_between(time_points, speed_errors, 0, alpha=0.3, color='red')\n",
    "            \n",
    "            axes[1, 2].set_xlabel('Time (s)', fontsize=label_fontsize)\n",
    "            axes[1, 2].set_ylabel('Error (m/s)', fontsize=label_fontsize)\n",
    "            axes[1, 2].legend(fontsize=legend_fontsize)\n",
    "            axes[1, 2].grid(True, alpha=0.3)\n",
    "            axes[1, 2].tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "        \n",
    "        fig.suptitle(f'Dynamic AR Model Validation - Driver {veh_id+1}', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbab5f4-d597-435a-b749-1c2b8c0e0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_results = run_dynamic_ar_calibration_only(\n",
    "     ar_idm_data, \n",
    "     ar_order=1, \n",
    "     tau_sec=5\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc47d4-5d00-4390-830d-1810c754bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_results = run_dynamic_ar_validation_only(\n",
    "     calibration_results, \n",
    "    n_posterior_samples=100,\n",
    "     ar_order=1,\n",
    "     tau_sec=5\n",
    " )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
