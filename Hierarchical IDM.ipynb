{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7d1b1-03e9-47a0-9dea-4841e67340e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386edc3-dab4-4d5c-82ae-0cf60882e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hierarchical_idm_model(train_data):\n",
    "\n",
    "    \n",
    "    vt = train_data['vt']\n",
    "    s = train_data['s']\n",
    "    dv = train_data['dv']\n",
    "    label_v = train_data['label_v']\n",
    "    id_idx = train_data['id_idx']\n",
    "    N_veh = train_data['n_vehicles']\n",
    "    \n",
    "    dt = 0.5\n",
    "    D = 5\n",
    "    DELTA = 4\n",
    "    \n",
    "    coords = {\n",
    "        \"veh_id\": np.arange(N_veh),\n",
    "        \"parameter\": np.arange(D)\n",
    "    }\n",
    "    \n",
    "    with pm.Model(coords=coords) as hierarchical_idm_model:\n",
    "        chol, _, _ = pm.LKJCholeskyCov('chol', n=D, eta=2.0, \n",
    "                                      sd_dist=pm.Exponential.dist(2, shape=D))\n",
    "        \n",
    "        log_mu_vmax = pm.Normal('log_mu_vmax', mu=0, sigma=0.1)\n",
    "        log_mu_dsafe = pm.Normal('log_mu_dsafe', mu=0, sigma=0.1)\n",
    "        log_mu_tsafe = pm.Normal('log_mu_tsafe', mu=0, sigma=0.1)\n",
    "        log_mu_amax = pm.Normal('log_mu_amax', mu=0, sigma=0.1)\n",
    "        \n",
    "        log_ratio_mu = pm.Normal('log_ratio_mu', mu=np.log(0.5), sigma=0.1)\n",
    "        ratio_mu = pm.Deterministic('ratio_mu', pt.exp(log_ratio_mu))\n",
    "        \n",
    "        log_ratio_raw = pm.Normal('log_ratio_raw', mu=0, sigma=0.1, shape=N_veh)\n",
    "        ratio_individual = pm.Deterministic('ratio_individual', ratio_mu * pt.exp(log_ratio_raw))\n",
    "        \n",
    "        vals_raw_vmax = pm.Normal('vals_raw_vmax', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_dsafe = pm.Normal('vals_raw_dsafe', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_tsafe = pm.Normal('vals_raw_tsafe', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_amax = pm.Normal('vals_raw_amax', mu=0, sigma=0.1, shape=N_veh)\n",
    "        \n",
    "        vals_raw_first4 = pm.Deterministic('vals_raw_first4', pt.stack([\n",
    "            vals_raw_vmax, vals_raw_dsafe, vals_raw_tsafe, vals_raw_amax\n",
    "        ], axis=1))\n",
    "        \n",
    "        log_mu_first4 = pt.stack([log_mu_vmax, log_mu_dsafe, log_mu_tsafe, log_mu_amax])\n",
    "        log_parameters_first4 = pm.Deterministic('log_parameters_first4', \n",
    "                                               log_mu_first4 + pt.dot(vals_raw_first4, chol[:4, :4].T))\n",
    "        parameters_first4 = pm.Deterministic('parameters_first4', pt.exp(log_parameters_first4))\n",
    "        \n",
    "        amin_individual = ratio_individual * parameters_first4[:, 3]\n",
    "        \n",
    "        parameters = pm.Deterministic('parameters', pt.stack([\n",
    "            parameters_first4[:, 0],\n",
    "            parameters_first4[:, 1],\n",
    "            parameters_first4[:, 2],\n",
    "            parameters_first4[:, 3],\n",
    "            amin_individual\n",
    "        ], axis=1), dims=('veh_id', 'parameter'))\n",
    "        \n",
    "        s_a_list = []\n",
    "        s_v_list = []\n",
    "        \n",
    "        for i in range(N_veh):\n",
    "            s_a_i = pm.Exponential(f's_a_{i}', lam=2000)\n",
    "            s_v_i = pm.Exponential(f's_v_{i}', lam=4000)\n",
    "            s_a_list.append(s_a_i)\n",
    "            s_v_list.append(s_v_i)\n",
    "        \n",
    "        for i in range(N_veh):\n",
    "            mask = (id_idx == i)\n",
    "            if np.sum(mask) > 5:\n",
    "                s_veh = s[mask]\n",
    "                vt_veh = vt[mask]\n",
    "                dv_veh = dv[mask]\n",
    "                label_veh = label_v[mask]\n",
    "                \n",
    "                vmax = 25 * parameters[i, 0]\n",
    "                dsafe = 2 * parameters[i, 1]\n",
    "                tsafe = 1.6 * parameters[i, 2]\n",
    "                amax = 1.5 * parameters[i, 3]\n",
    "                amin = 1.5 * parameters[i, 4]\n",
    "                \n",
    "                sn = dsafe + vt_veh * tsafe + \\\n",
    "                     vt_veh * dv_veh / (2 * pm.math.sqrt(amax * amin))\n",
    "                a_idm = amax * (1 - (vt_veh / vmax) ** DELTA - (sn / s_veh) ** 2)\n",
    "                \n",
    "                mean_speed = vt_veh + a_idm * dt\n",
    "                \n",
    "                total_sigma = pm.math.sqrt((s_a_list[i] * dt) ** 2 + s_v_list[i] ** 2)\n",
    "                pm.Normal(f'obs_{i}', mu=mean_speed, \n",
    "                         sigma=total_sigma,\n",
    "                         observed=label_veh)\n",
    "\n",
    "        try:\n",
    "            trace = pm.sample(\n",
    "                draws=600, \n",
    "                tune=600, \n",
    "                random_seed=42, \n",
    "                chains=2,\n",
    "                target_accept=0.9, \n",
    "                return_inferencedata=True\n",
    "            )\n",
    "            print(\"Hierarchical IDM model training completed!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Sampling error: {e}\")\n",
    "            print(\"Trying more conservative sampling settings...\")\n",
    "            trace = pm.sample(\n",
    "                draws=1500, \n",
    "                tune=1000, \n",
    "                random_seed=42,\n",
    "                chains=2, \n",
    "                target_accept=0.8,\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "            print(\"Hierarchical IDM model training completed (using conservative settings)!\")\n",
    "    \n",
    "    return trace, hierarchical_idm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7f7f7-c506-4929-8f24-093084305867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    nrmse = rmse / (np.max(y_true) - np.min(y_true))\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'nrmse': nrmse\n",
    "    }\n",
    "\n",
    "def robust_smooth_acceleration(velocity, dt=0.5, window_size=7, poly_order=2):\n",
    "    if len(velocity) < window_size:\n",
    "        acceleration = np.gradient(velocity, dt)\n",
    "        return acceleration\n",
    "    \n",
    "    try:\n",
    "        acceleration = signal.savitzky_golay(velocity, window_length=window_size, \n",
    "                                           polyorder=poly_order, deriv=1, delta=dt)\n",
    "        \n",
    "        if len(acceleration) > 10:\n",
    "            x_fit = np.arange(5) * dt\n",
    "            y_fit = acceleration[5:10]\n",
    "            if len(y_fit) >= 2:\n",
    "                slope, intercept = np.polyfit(x_fit[:len(y_fit)], y_fit, 1)\n",
    "                for i in range(5):\n",
    "                    acceleration[i] = intercept + slope * (i * dt)\n",
    "            \n",
    "            x_fit = np.arange(5) * dt\n",
    "            y_fit = acceleration[-10:-5]\n",
    "            if len(y_fit) >= 2:\n",
    "                slope, intercept = np.polyfit(x_fit[:len(y_fit)], y_fit, 1)\n",
    "                for i in range(5):\n",
    "                    acceleration[-(5-i)] = intercept + slope * ((4-i) * dt)\n",
    "        \n",
    "        return acceleration\n",
    "        \n",
    "    except:\n",
    "        acceleration = np.zeros_like(velocity)\n",
    "        for i in range(1, len(velocity)-1):\n",
    "            acceleration[i] = (velocity[i+1] - velocity[i-1]) / (2 * dt)\n",
    "        \n",
    "        if len(velocity) > 1:\n",
    "            acceleration[0] = (velocity[1] - velocity[0]) / dt\n",
    "            acceleration[-1] = (velocity[-1] - velocity[-2]) / dt\n",
    "        \n",
    "        window = min(5, len(acceleration))\n",
    "        acceleration = np.convolve(acceleration, np.ones(window)/window, mode='same')\n",
    "        \n",
    "        return acceleration\n",
    "\n",
    "def calculate_initial_acceleration(velocity, dt=0.5, method='savitzky_golay'):\n",
    "    if len(velocity) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    if method == 'savitzky_golay':\n",
    "        acc_all = robust_smooth_acceleration(velocity, dt)\n",
    "        return acc_all[0]\n",
    "    \n",
    "    elif method == 'robust_fit':\n",
    "        n_points = min(5, len(velocity))\n",
    "        t_points = np.arange(n_points) * dt\n",
    "        v_points = velocity[:n_points]\n",
    "        \n",
    "        slope, intercept = np.polyfit(t_points, v_points, 1)\n",
    "        return slope\n",
    "    \n",
    "    elif method == 'physical_constrained':\n",
    "        if len(velocity) >= 4:\n",
    "            weights = np.array([0.1, 0.2, 0.3, 0.4])[:len(velocity)]\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            t_points = np.arange(len(velocity)) * dt\n",
    "            A = np.vstack([t_points, np.ones(len(t_points))]).T\n",
    "            W = np.diag(weights)\n",
    "            slope, intercept = np.linalg.lstsq(A.T @ W @ A, A.T @ W @ velocity, rcond=None)[0]\n",
    "            return slope\n",
    "        else:\n",
    "            return (velocity[1] - velocity[0]) / dt\n",
    "    \n",
    "    else:\n",
    "        initial_acc = (velocity[1] - velocity[0]) / dt\n",
    "        return np.clip(initial_acc, -3.0, 3.0)\n",
    "\n",
    "def improved_robust_acceleration(velocity, dt=0.5, window_size=7, poly_order=2):\n",
    "    acceleration = robust_smooth_acceleration(velocity, dt, window_size, poly_order)\n",
    "    acceleration = np.clip(acceleration, -3.0, 3.0)\n",
    "    \n",
    "    if len(acceleration) > 10:\n",
    "        acceleration[:3] = np.mean(acceleration[:5])\n",
    "        acceleration[-3:] = np.mean(acceleration[-5:])\n",
    "    \n",
    "    return acceleration\n",
    "\n",
    "def split_data_for_ar_idm(ar_idm_data, train_ratio=0.7):\n",
    "    vt = ar_idm_data['vt']\n",
    "    s = ar_idm_data['s']\n",
    "    dv = ar_idm_data['dv']\n",
    "    label_v = ar_idm_data['label_v']\n",
    "    id_idx = ar_idm_data['id_idx']\n",
    "    \n",
    "    unique_vehicles = np.unique(id_idx)\n",
    "    \n",
    "    train_data = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    val_data = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        mask = (id_idx == veh_id)\n",
    "        n_points = np.sum(mask)\n",
    "        \n",
    "        if n_points < 20:\n",
    "            continue\n",
    "            \n",
    "        split_point = int(n_points * train_ratio)\n",
    "        \n",
    "        train_mask = np.zeros_like(mask, dtype=bool)\n",
    "        train_indices = np.where(mask)[0][:split_point]\n",
    "        train_mask[train_indices] = True\n",
    "        \n",
    "        val_mask = np.zeros_like(mask, dtype=bool)\n",
    "        val_indices = np.where(mask)[0][split_point:]\n",
    "        val_mask[val_indices] = True\n",
    "        \n",
    "        train_data['vt'] = np.concatenate([train_data['vt'], vt[train_mask]])\n",
    "        train_data['s'] = np.concatenate([train_data['s'], s[train_mask]])\n",
    "        train_data['dv'] = np.concatenate([train_data['dv'], dv[train_mask]])\n",
    "        train_data['label_v'] = np.concatenate([train_data['label_v'], label_v[train_mask]])\n",
    "        train_data['id_idx'] = np.concatenate([train_data['id_idx'], np.full(np.sum(train_mask), veh_id)])\n",
    "        \n",
    "        if np.sum(train_mask) > 0:\n",
    "            train_data['tracks'][veh_id] = {\n",
    "                'last_vt': vt[train_mask][-1],\n",
    "                'last_s': s[train_mask][-1],\n",
    "                'last_dv': dv[train_mask][-1] if len(dv[train_mask]) > 0 else 0.0\n",
    "            }\n",
    "        \n",
    "        val_data['vt'] = np.concatenate([val_data['vt'], vt[val_mask]])\n",
    "        val_data['s'] = np.concatenate([val_data['s'], s[val_mask]])\n",
    "        val_data['dv'] = np.concatenate([val_data['dv'], dv[val_mask]])\n",
    "        val_data['label_v'] = np.concatenate([val_data['label_v'], label_v[val_mask]])\n",
    "        val_data['id_idx'] = np.concatenate([val_data['id_idx'], np.full(np.sum(val_mask), veh_id)])\n",
    "    \n",
    "    print(f\"Training set: {len(train_data['vt'])} data points\")\n",
    "    print(f\"Validation set: {len(val_data['vt'])} data points\")\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffa7cc-6962-42d2-9b0a-497d67b6e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_hierarchical_idm_model_comprehensive_improved(trace, model, train_data, val_data, n_samples=100):\n",
    "    \n",
    "    vt_val = val_data['vt']\n",
    "    s_val = val_data['s']\n",
    "    dv_val = val_data['dv']\n",
    "    label_val = val_data['label_v']\n",
    "    id_idx_val = val_data['id_idx']\n",
    "    N_veh = val_data['n_vehicles']\n",
    "    \n",
    "    dt = 0.5\n",
    "    D = 5\n",
    "    DELTA = 4\n",
    "    \n",
    "\n",
    "    print(f\"Drawing {n_samples} posterior samples...\")\n",
    "    with model:\n",
    "        posterior_samples = pm.sample_posterior_predictive(\n",
    "            trace, \n",
    "            var_names=['parameters', 's_a_0', 's_a_1', 's_a_2', 's_a_3'], \n",
    "            samples=n_samples,\n",
    "            random_seed=42\n",
    "        )\n",
    "\n",
    "    parameters_samples = posterior_samples['parameters']  # shape: (n_samples, N_veh, D)\n",
    "    \n",
    "\n",
    "    all_samples_speed_predictions = []\n",
    "    all_samples_spacing_predictions = []\n",
    "    all_samples_acceleration_predictions = []\n",
    "    \n",
    "    print(\"Generating predictions for each posterior sample...\")\n",
    "    for sample_idx in range(n_samples):\n",
    "        speed_predictions = np.zeros_like(vt_val)\n",
    "        spacing_predictions = np.zeros_like(s_val)\n",
    "        acceleration_predictions = np.zeros_like(vt_val)\n",
    "        \n",
    "\n",
    "        params_sample = parameters_samples[sample_idx]  # shape: (N_veh, D)\n",
    "        \n",
    "        for veh_id in range(N_veh):\n",
    "            mask = (id_idx_val == veh_id)\n",
    "            if np.sum(mask) > 0:\n",
    "                vt_veh = vt_val[mask]\n",
    "                s_veh = s_val[mask]\n",
    "                dv_veh = dv_val[mask]\n",
    "                               \n",
    "                vmax = 25 * params_sample[veh_id, 0]\n",
    "                dsafe = 2 * params_sample[veh_id, 1]\n",
    "                tsafe = 1.6 * params_sample[veh_id, 2]\n",
    "                amax = 1.5 * params_sample[veh_id, 3]\n",
    "                amin = 1.5 * params_sample[veh_id, 4]\n",
    "                \n",
    "\n",
    "                sn = dsafe + vt_veh * tsafe + vt_veh * dv_veh / (2 * np.sqrt(amax * amin))\n",
    "                a_idm = amax * (1 - (vt_veh / vmax) ** DELTA - (sn / s_veh) ** 2)\n",
    "                \n",
    "                \n",
    "                speed_pred = vt_veh + a_idm * dt\n",
    "                \n",
    "               \n",
    "                speed_predictions[mask] = speed_pred\n",
    "                spacing_predictions[mask] = s_veh  \n",
    "                acceleration_predictions[mask] = a_idm\n",
    "        \n",
    "        all_samples_speed_predictions.append(speed_predictions)\n",
    "        all_samples_spacing_predictions.append(spacing_predictions)\n",
    "        all_samples_acceleration_predictions.append(acceleration_predictions)\n",
    "    \n",
    "\n",
    "    real_acceleration = np.zeros_like(vt_val)\n",
    "    valid_indices = []\n",
    "    \n",
    "    for veh_id in range(N_veh):\n",
    "        mask = (id_idx_val == veh_id)\n",
    "        if np.sum(mask) > 2:\n",
    "            vt_veh = vt_val[mask]\n",
    "            \n",
    "            acc_veh = improved_robust_acceleration(vt_veh, dt=dt)\n",
    "            real_acceleration[mask] = acc_veh\n",
    "            valid_indices.extend(np.where(mask)[0])\n",
    "    \n",
    "\n",
    "    all_samples_speed_predictions = np.array(all_samples_speed_predictions)\n",
    "    all_samples_spacing_predictions = np.array(all_samples_spacing_predictions)\n",
    "    all_samples_acceleration_predictions = np.array(all_samples_acceleration_predictions)\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "    mean_speed_pred = np.mean(all_samples_speed_predictions, axis=0)\n",
    "    mean_acceleration_pred = np.mean(all_samples_acceleration_predictions, axis=0)\n",
    "    \n",
    "    speed_metrics = calculate_metrics(label_val, mean_speed_pred)\n",
    "    acceleration_metrics = calculate_metrics(real_acceleration[valid_indices], \n",
    "                                           mean_acceleration_pred[valid_indices])\n",
    "    spacing_metrics = calculate_metrics(s_val, np.mean(all_samples_spacing_predictions, axis=0))\n",
    "    \n",
    " \n",
    "    vehicle_metrics = {}\n",
    "    unique_vehicles = np.unique(id_idx_val)\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        mask = (id_idx_val == veh_id)\n",
    "        if np.sum(mask) > 0:\n",
    "            veh_speed_true = label_val[mask]\n",
    "            veh_speed_pred = mean_speed_pred[mask]\n",
    "            veh_accel_true = real_acceleration[mask]\n",
    "            veh_accel_pred = mean_acceleration_pred[mask]\n",
    "            veh_spacing_true = s_val[mask]\n",
    "            veh_spacing_pred = np.mean(all_samples_spacing_predictions[:, mask], axis=0)\n",
    "            \n",
    "            vehicle_metrics[veh_id] = {\n",
    "                'speed': calculate_metrics(veh_speed_true, veh_speed_pred),\n",
    "                'acceleration': calculate_metrics(veh_accel_true, veh_accel_pred),\n",
    "                'spacing': calculate_metrics(veh_spacing_true, veh_spacing_pred)\n",
    "            }\n",
    "    \n",
    "    individual_params = parameters_samples.reshape(-1, D)\n",
    "    \n",
    "    validation_results = {\n",
    "        'all_samples_speed_predictions': all_samples_speed_predictions,\n",
    "        'all_samples_spacing_predictions': all_samples_spacing_predictions,\n",
    "        'all_samples_acceleration_predictions': all_samples_acceleration_predictions,\n",
    "        'real_acceleration': real_acceleration,\n",
    "        'valid_indices': valid_indices,\n",
    "        'n_samples': n_samples,\n",
    "        'speed_metrics': speed_metrics,\n",
    "        'acceleration_metrics': acceleration_metrics,\n",
    "        'spacing_metrics': spacing_metrics,\n",
    "        'vehicle_metrics': vehicle_metrics,\n",
    "        'individual_params': individual_params,\n",
    "        'mean_predictions': {\n",
    "            'speed': mean_speed_pred,\n",
    "            'acceleration': mean_acceleration_pred,\n",
    "            'spacing': np.mean(all_samples_spacing_predictions, axis=0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "\n",
    "\n",
    "def print_hierarchical_idm_validation_summary(validation_results):\n",
    "  \n",
    "    \n",
    "    speed_metrics = validation_results['speed_metrics']\n",
    "    acceleration_metrics = validation_results['acceleration_metrics']\n",
    "    spacing_metrics = validation_results['spacing_metrics']\n",
    "    vehicle_metrics = validation_results['vehicle_metrics']\n",
    "    \n",
    "    print(f\"\\nOVERALL PERFORMANCE METRICS:\")\n",
    "    print(f\"Speed Prediction:\")\n",
    "    print(f\"  - RMSE: {speed_metrics['rmse']:.4f} m/s\")\n",
    "    print(f\"  - MAE: {speed_metrics['mae']:.4f} m/s\")\n",
    "    print(f\"  - NRMSE: {speed_metrics['nrmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nAcceleration Prediction:\")\n",
    "    print(f\"  - RMSE: {acceleration_metrics['rmse']:.4f} m/s²\")\n",
    "    print(f\"  - MAE: {acceleration_metrics['mae']:.4f} m/s²\")\n",
    "    print(f\"  - NRMSE: {acceleration_metrics['nrmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSpacing Prediction:\")\n",
    "    print(f\"  - RMSE: {spacing_metrics['rmse']:.4f} m\")\n",
    "    print(f\"  - MAE: {spacing_metrics['mae']:.4f} m\")\n",
    "    print(f\"  - NRMSE: {spacing_metrics['nrmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nVEHICLE-LEVEL PERFORMANCE:\")\n",
    "    for veh_id, metrics in vehicle_metrics.items():\n",
    "        print(f\"\\nVehicle {veh_id}:\")\n",
    "        print(f\"  Speed - RMSE: {metrics['speed']['rmse']:.4f} m/s, MAE: {metrics['speed']['mae']:.4f} m/s\")\n",
    "        print(f\"  Acceleration - RMSE: {metrics['acceleration']['rmse']:.4f} m/s², MAE: {metrics['acceleration']['mae']:.4f} m/s²\")\n",
    "        print(f\"  Spacing - RMSE: {metrics['spacing']['rmse']:.4f} m, MAE: {metrics['spacing']['mae']:.4f} m\")\n",
    "    \n",
    "\n",
    "    individual_params = validation_results['individual_params']\n",
    "    param_names = ['vmax', 'dsafe', 'tsafe', 'amax', 'amin']\n",
    "    param_scales = [25, 2, 1.6, 1.5, 1.5]\n",
    "    \n",
    "    print(f\"\\nPARAMETER POSTERIOR STATISTICS:\")\n",
    "    for i, name in enumerate(param_names):\n",
    "        scaled_params = individual_params[:, i] * param_scales[i]\n",
    "        print(f\"  {name}: Mean = {np.mean(scaled_params):.3f}, Std = {np.std(scaled_params):.3f}, \"\n",
    "              f\"95% CI = [{np.percentile(scaled_params, 2.5):.3f}, {np.percentile(scaled_params, 97.5):.3f}]\")\n",
    "\n",
    "def run_hierarchical_idm_calibration_only(ar_idm_data):\n",
    "\n",
    "    \n",
    "    train_data, val_data = split_data_for_ar_idm(ar_idm_data, train_ratio=0.7)\n",
    "    \n",
    "\n",
    "    trace, model = train_hierarchical_idm_model(train_data)\n",
    "\n",
    "    return {\n",
    "        'trace': trace,\n",
    "        'model': model,\n",
    "        'train_data': train_data,\n",
    "        'val_data': val_data\n",
    "    }\n",
    "\n",
    "def run_hierarchical_idm_validation_only(calibration_results, n_posterior_samples=100):\n",
    "\n",
    "    \n",
    "    trace = calibration_results['trace']\n",
    "    model = calibration_results['model']\n",
    "    train_data = calibration_results['train_data']\n",
    "    val_data = calibration_results['val_data']\n",
    "    \n",
    "\n",
    "    validation_results = validate_hierarchical_idm_model_comprehensive_improved(\n",
    "        trace, model, train_data, val_data, n_samples=n_posterior_samples\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    print_hierarchical_idm_validation_summary(validation_results)\n",
    "    \n",
    "    \n",
    "    return validation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184b7fa-12ec-440b-8d5a-d67b4053505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_results = run_hierarchical_idm_calibration_only(ar_idm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ee1c7-49ab-4fc6-9e20-15c531106280",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = run_hierarchical_idm_validation_only(calibration_results, n_posterior_samples=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
