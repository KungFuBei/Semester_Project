{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386edc3-dab4-4d5c-82ae-0cf60882e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hierarchical_idm_model(train_data):\n",
    "    \"\"\"\n",
    "    Train hierarchical IDM model on training set with hierarchical structure only (no AR component)\n",
    "    \"\"\"\n",
    "    print(\"Starting hierarchical IDM model training...\")\n",
    "    \n",
    "    vt = train_data['vt']\n",
    "    s = train_data['s']\n",
    "    dv = train_data['dv']\n",
    "    label_v = train_data['label_v']\n",
    "    id_idx = train_data['id_idx']\n",
    "    N_veh = train_data['n_vehicles']\n",
    "    \n",
    "    dt = 0.5\n",
    "    D = 5\n",
    "    DELTA = 4\n",
    "    \n",
    "    coords = {\n",
    "        \"veh_id\": np.arange(N_veh),\n",
    "        \"parameter\": np.arange(D)\n",
    "    }\n",
    "    \n",
    "    with pm.Model(coords=coords) as hierarchical_idm_model:\n",
    "        chol, _, _ = pm.LKJCholeskyCov('chol', n=D, eta=2.0, \n",
    "                                      sd_dist=pm.Exponential.dist(2, shape=D))\n",
    "        \n",
    "        log_mu_vmax = pm.Normal('log_mu_vmax', mu=0, sigma=0.1)\n",
    "        log_mu_dsafe = pm.Normal('log_mu_dsafe', mu=0, sigma=0.1)\n",
    "        log_mu_tsafe = pm.Normal('log_mu_tsafe', mu=0, sigma=0.1)\n",
    "        log_mu_amax = pm.Normal('log_mu_amax', mu=0, sigma=0.1)\n",
    "        \n",
    "        log_ratio_mu = pm.Normal('log_ratio_mu', mu=np.log(0.5), sigma=0.1)\n",
    "        ratio_mu = pm.Deterministic('ratio_mu', pt.exp(log_ratio_mu))\n",
    "        \n",
    "        log_ratio_raw = pm.Normal('log_ratio_raw', mu=0, sigma=0.1, shape=N_veh)\n",
    "        ratio_individual = pm.Deterministic('ratio_individual', ratio_mu * pt.exp(log_ratio_raw))\n",
    "        \n",
    "        vals_raw_vmax = pm.Normal('vals_raw_vmax', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_dsafe = pm.Normal('vals_raw_dsafe', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_tsafe = pm.Normal('vals_raw_tsafe', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_amax = pm.Normal('vals_raw_amax', mu=0, sigma=0.1, shape=N_veh)\n",
    "        \n",
    "        vals_raw_first4 = pm.Deterministic('vals_raw_first4', pt.stack([\n",
    "            vals_raw_vmax, vals_raw_dsafe, vals_raw_tsafe, vals_raw_amax\n",
    "        ], axis=1))\n",
    "        \n",
    "        log_mu_first4 = pt.stack([log_mu_vmax, log_mu_dsafe, log_mu_tsafe, log_mu_amax])\n",
    "        log_parameters_first4 = pm.Deterministic('log_parameters_first4', \n",
    "                                               log_mu_first4 + pt.dot(vals_raw_first4, chol[:4, :4].T))\n",
    "        parameters_first4 = pm.Deterministic('parameters_first4', pt.exp(log_parameters_first4))\n",
    "        \n",
    "        amin_individual = ratio_individual * parameters_first4[:, 3]\n",
    "        \n",
    "        parameters = pm.Deterministic('parameters', pt.stack([\n",
    "            parameters_first4[:, 0],\n",
    "            parameters_first4[:, 1],\n",
    "            parameters_first4[:, 2],\n",
    "            parameters_first4[:, 3],\n",
    "            amin_individual\n",
    "        ], axis=1), dims=('veh_id', 'parameter'))\n",
    "        \n",
    "        s_a_list = []\n",
    "        s_v_list = []\n",
    "        \n",
    "        for i in range(N_veh):\n",
    "            s_a_i = pm.Exponential(f's_a_{i}', lam=2000)\n",
    "            s_v_i = pm.Exponential(f's_v_{i}', lam=4000)\n",
    "            s_a_list.append(s_a_i)\n",
    "            s_v_list.append(s_v_i)\n",
    "        \n",
    "        for i in range(N_veh):\n",
    "            mask = (id_idx == i)\n",
    "            if np.sum(mask) > 5:\n",
    "                s_veh = s[mask]\n",
    "                vt_veh = vt[mask]\n",
    "                dv_veh = dv[mask]\n",
    "                label_veh = label_v[mask]\n",
    "                \n",
    "                vmax = 25 * parameters[i, 0]\n",
    "                dsafe = 2 * parameters[i, 1]\n",
    "                tsafe = 1.6 * parameters[i, 2]\n",
    "                amax = 1.5 * parameters[i, 3]\n",
    "                amin = 1.5 * parameters[i, 4]\n",
    "                \n",
    "                sn = dsafe + vt_veh * tsafe + \\\n",
    "                     vt_veh * dv_veh / (2 * pm.math.sqrt(amax * amin))\n",
    "                a_idm = amax * (1 - (vt_veh / vmax) ** DELTA - (sn / s_veh) ** 2)\n",
    "                \n",
    "                mean_speed = vt_veh + a_idm * dt\n",
    "                \n",
    "                total_sigma = pm.math.sqrt((s_a_list[i] * dt) ** 2 + s_v_list[i] ** 2)\n",
    "                pm.Normal(f'obs_{i}', mu=mean_speed, \n",
    "                         sigma=total_sigma,\n",
    "                         observed=label_veh)\n",
    "\n",
    "        try:\n",
    "            trace = pm.sample(\n",
    "                draws=600, \n",
    "                tune=600, \n",
    "                random_seed=42, \n",
    "                chains=2,\n",
    "                target_accept=0.9, \n",
    "                return_inferencedata=True\n",
    "            )\n",
    "            print(\"Hierarchical IDM model training completed!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Sampling error: {e}\")\n",
    "            print(\"Trying more conservative sampling settings...\")\n",
    "            trace = pm.sample(\n",
    "                draws=1500, \n",
    "                tune=1000, \n",
    "                random_seed=42,\n",
    "                chains=2, \n",
    "                target_accept=0.8,\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "            print(\"Hierarchical IDM model training completed (using conservative settings)!\")\n",
    "    \n",
    "    return trace, hierarchical_idm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7f7f7-c506-4929-8f24-093084305867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    nrmse = rmse / (np.max(y_true) - np.min(y_true))\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'nrmse': nrmse\n",
    "    }\n",
    "\n",
    "def robust_smooth_acceleration(velocity, dt=0.5, window_size=7, poly_order=2):\n",
    "    if len(velocity) < window_size:\n",
    "        acceleration = np.gradient(velocity, dt)\n",
    "        return acceleration\n",
    "    \n",
    "    try:\n",
    "        acceleration = signal.savitzky_golay(velocity, window_length=window_size, \n",
    "                                           polyorder=poly_order, deriv=1, delta=dt)\n",
    "        \n",
    "        if len(acceleration) > 10:\n",
    "            x_fit = np.arange(5) * dt\n",
    "            y_fit = acceleration[5:10]\n",
    "            if len(y_fit) >= 2:\n",
    "                slope, intercept = np.polyfit(x_fit[:len(y_fit)], y_fit, 1)\n",
    "                for i in range(5):\n",
    "                    acceleration[i] = intercept + slope * (i * dt)\n",
    "            \n",
    "            x_fit = np.arange(5) * dt\n",
    "            y_fit = acceleration[-10:-5]\n",
    "            if len(y_fit) >= 2:\n",
    "                slope, intercept = np.polyfit(x_fit[:len(y_fit)], y_fit, 1)\n",
    "                for i in range(5):\n",
    "                    acceleration[-(5-i)] = intercept + slope * ((4-i) * dt)\n",
    "        \n",
    "        return acceleration\n",
    "        \n",
    "    except:\n",
    "        acceleration = np.zeros_like(velocity)\n",
    "        for i in range(1, len(velocity)-1):\n",
    "            acceleration[i] = (velocity[i+1] - velocity[i-1]) / (2 * dt)\n",
    "        \n",
    "        if len(velocity) > 1:\n",
    "            acceleration[0] = (velocity[1] - velocity[0]) / dt\n",
    "            acceleration[-1] = (velocity[-1] - velocity[-2]) / dt\n",
    "        \n",
    "        window = min(5, len(acceleration))\n",
    "        acceleration = np.convolve(acceleration, np.ones(window)/window, mode='same')\n",
    "        \n",
    "        return acceleration\n",
    "\n",
    "def calculate_initial_acceleration(velocity, dt=0.5, method='savitzky_golay'):\n",
    "    if len(velocity) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    if method == 'savitzky_golay':\n",
    "        acc_all = robust_smooth_acceleration(velocity, dt)\n",
    "        return acc_all[0]\n",
    "    \n",
    "    elif method == 'robust_fit':\n",
    "        n_points = min(5, len(velocity))\n",
    "        t_points = np.arange(n_points) * dt\n",
    "        v_points = velocity[:n_points]\n",
    "        \n",
    "        slope, intercept = np.polyfit(t_points, v_points, 1)\n",
    "        return slope\n",
    "    \n",
    "    elif method == 'physical_constrained':\n",
    "        if len(velocity) >= 4:\n",
    "            weights = np.array([0.1, 0.2, 0.3, 0.4])[:len(velocity)]\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            t_points = np.arange(len(velocity)) * dt\n",
    "            A = np.vstack([t_points, np.ones(len(t_points))]).T\n",
    "            W = np.diag(weights)\n",
    "            slope, intercept = np.linalg.lstsq(A.T @ W @ A, A.T @ W @ velocity, rcond=None)[0]\n",
    "            return slope\n",
    "        else:\n",
    "            return (velocity[1] - velocity[0]) / dt\n",
    "    \n",
    "    else:\n",
    "        initial_acc = (velocity[1] - velocity[0]) / dt\n",
    "        return np.clip(initial_acc, -3.0, 3.0)\n",
    "\n",
    "def improved_robust_acceleration(velocity, dt=0.5, window_size=7, poly_order=2):\n",
    "    acceleration = robust_smooth_acceleration(velocity, dt, window_size, poly_order)\n",
    "    acceleration = np.clip(acceleration, -3.0, 3.0)\n",
    "    \n",
    "    if len(acceleration) > 10:\n",
    "        acceleration[:3] = np.mean(acceleration[:5])\n",
    "        acceleration[-3:] = np.mean(acceleration[-5:])\n",
    "    \n",
    "    return acceleration\n",
    "\n",
    "def split_data_for_ar_idm(ar_idm_data, train_ratio=0.7):\n",
    "    vt = ar_idm_data['vt']\n",
    "    s = ar_idm_data['s']\n",
    "    dv = ar_idm_data['dv']\n",
    "    label_v = ar_idm_data['label_v']\n",
    "    id_idx = ar_idm_data['id_idx']\n",
    "    \n",
    "    unique_vehicles = np.unique(id_idx)\n",
    "    \n",
    "    train_data = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    val_data = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        mask = (id_idx == veh_id)\n",
    "        n_points = np.sum(mask)\n",
    "        \n",
    "        if n_points < 20:\n",
    "            continue\n",
    "            \n",
    "        split_point = int(n_points * train_ratio)\n",
    "        \n",
    "        train_mask = np.zeros_like(mask, dtype=bool)\n",
    "        train_indices = np.where(mask)[0][:split_point]\n",
    "        train_mask[train_indices] = True\n",
    "        \n",
    "        val_mask = np.zeros_like(mask, dtype=bool)\n",
    "        val_indices = np.where(mask)[0][split_point:]\n",
    "        val_mask[val_indices] = True\n",
    "        \n",
    "        train_data['vt'] = np.concatenate([train_data['vt'], vt[train_mask]])\n",
    "        train_data['s'] = np.concatenate([train_data['s'], s[train_mask]])\n",
    "        train_data['dv'] = np.concatenate([train_data['dv'], dv[train_mask]])\n",
    "        train_data['label_v'] = np.concatenate([train_data['label_v'], label_v[train_mask]])\n",
    "        train_data['id_idx'] = np.concatenate([train_data['id_idx'], np.full(np.sum(train_mask), veh_id)])\n",
    "        \n",
    "        if np.sum(train_mask) > 0:\n",
    "            train_data['tracks'][veh_id] = {\n",
    "                'last_vt': vt[train_mask][-1],\n",
    "                'last_s': s[train_mask][-1],\n",
    "                'last_dv': dv[train_mask][-1] if len(dv[train_mask]) > 0 else 0.0\n",
    "            }\n",
    "        \n",
    "        val_data['vt'] = np.concatenate([val_data['vt'], vt[val_mask]])\n",
    "        val_data['s'] = np.concatenate([val_data['s'], s[val_mask]])\n",
    "        val_data['dv'] = np.concatenate([val_data['dv'], dv[val_mask]])\n",
    "        val_data['label_v'] = np.concatenate([val_data['label_v'], label_v[val_mask]])\n",
    "        val_data['id_idx'] = np.concatenate([val_data['id_idx'], np.full(np.sum(val_mask), veh_id)])\n",
    "    \n",
    "    print(f\"Training set: {len(train_data['vt'])} data points\")\n",
    "    print(f\"Validation set: {len(val_data['vt'])} data points\")\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffa7cc-6962-42d2-9b0a-497d67b6e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from collections import deque\n",
    "\n",
    "def validate_hierarchical_idm_model_comprehensive_improved(trace, model, train_data, val_data, n_samples=100):\n",
    "    \"\"\"\n",
    "    Comprehensive validation for hierarchical IDM model with improved posterior sampling\n",
    "    \"\"\"\n",
    "    print(\"Starting comprehensive hierarchical IDM model validation...\")\n",
    "    \n",
    "    vt_val = val_data['vt']\n",
    "    s_val = val_data['s']\n",
    "    dv_val = val_data['dv']\n",
    "    label_val = val_data['label_v']\n",
    "    id_idx_val = val_data['id_idx']\n",
    "    N_veh = val_data['n_vehicles']\n",
    "    \n",
    "    dt = 0.5\n",
    "    D = 5\n",
    "    DELTA = 4\n",
    "    \n",
    "    # Sample from posterior\n",
    "    print(f\"Drawing {n_samples} posterior samples...\")\n",
    "    with model:\n",
    "        posterior_samples = pm.sample_posterior_predictive(\n",
    "            trace, \n",
    "            var_names=['parameters', 's_a_0', 's_a_1', 's_a_2', 's_a_3'], \n",
    "            samples=n_samples,\n",
    "            random_seed=42\n",
    "        )\n",
    "    \n",
    "    # Extract parameters\n",
    "    parameters_samples = posterior_samples['parameters']  # shape: (n_samples, N_veh, D)\n",
    "    \n",
    "    # Initialize arrays for predictions\n",
    "    all_samples_speed_predictions = []\n",
    "    all_samples_spacing_predictions = []\n",
    "    all_samples_acceleration_predictions = []\n",
    "    \n",
    "    print(\"Generating predictions for each posterior sample...\")\n",
    "    for sample_idx in range(n_samples):\n",
    "        speed_predictions = np.zeros_like(vt_val)\n",
    "        spacing_predictions = np.zeros_like(s_val)\n",
    "        acceleration_predictions = np.zeros_like(vt_val)\n",
    "        \n",
    "        # Get parameters for this sample\n",
    "        params_sample = parameters_samples[sample_idx]  # shape: (N_veh, D)\n",
    "        \n",
    "        for veh_id in range(N_veh):\n",
    "            mask = (id_idx_val == veh_id)\n",
    "            if np.sum(mask) > 0:\n",
    "                vt_veh = vt_val[mask]\n",
    "                s_veh = s_val[mask]\n",
    "                dv_veh = dv_val[mask]\n",
    "                \n",
    "                # Get individual parameters for this vehicle\n",
    "                vmax = 25 * params_sample[veh_id, 0]\n",
    "                dsafe = 2 * params_sample[veh_id, 1]\n",
    "                tsafe = 1.6 * params_sample[veh_id, 2]\n",
    "                amax = 1.5 * params_sample[veh_id, 3]\n",
    "                amin = 1.5 * params_sample[veh_id, 4]\n",
    "                \n",
    "                # Calculate IDM acceleration\n",
    "                sn = dsafe + vt_veh * tsafe + vt_veh * dv_veh / (2 * np.sqrt(amax * amin))\n",
    "                a_idm = amax * (1 - (vt_veh / vmax) ** DELTA - (sn / s_veh) ** 2)\n",
    "                \n",
    "                # Predict speed (no AR correction)\n",
    "                speed_pred = vt_veh + a_idm * dt\n",
    "                \n",
    "                # Store predictions\n",
    "                speed_predictions[mask] = speed_pred\n",
    "                spacing_predictions[mask] = s_veh  # Spacing remains the same as input\n",
    "                acceleration_predictions[mask] = a_idm\n",
    "        \n",
    "        all_samples_speed_predictions.append(speed_predictions)\n",
    "        all_samples_spacing_predictions.append(spacing_predictions)\n",
    "        all_samples_acceleration_predictions.append(acceleration_predictions)\n",
    "    \n",
    "    # Calculate real acceleration from validation data\n",
    "    print(\"Calculating real acceleration from validation data...\")\n",
    "    real_acceleration = np.zeros_like(vt_val)\n",
    "    valid_indices = []\n",
    "    \n",
    "    for veh_id in range(N_veh):\n",
    "        mask = (id_idx_val == veh_id)\n",
    "        if np.sum(mask) > 2:\n",
    "            vt_veh = vt_val[mask]\n",
    "            # Use improved acceleration calculation\n",
    "            acc_veh = improved_robust_acceleration(vt_veh, dt=dt)\n",
    "            real_acceleration[mask] = acc_veh\n",
    "            valid_indices.extend(np.where(mask)[0])\n",
    "    \n",
    "    # Convert to arrays\n",
    "    all_samples_speed_predictions = np.array(all_samples_speed_predictions)\n",
    "    all_samples_spacing_predictions = np.array(all_samples_spacing_predictions)\n",
    "    all_samples_acceleration_predictions = np.array(all_samples_acceleration_predictions)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"Calculating performance metrics...\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    mean_speed_pred = np.mean(all_samples_speed_predictions, axis=0)\n",
    "    mean_acceleration_pred = np.mean(all_samples_acceleration_predictions, axis=0)\n",
    "    \n",
    "    speed_metrics = calculate_metrics(label_val, mean_speed_pred)\n",
    "    acceleration_metrics = calculate_metrics(real_acceleration[valid_indices], \n",
    "                                           mean_acceleration_pred[valid_indices])\n",
    "    spacing_metrics = calculate_metrics(s_val, np.mean(all_samples_spacing_predictions, axis=0))\n",
    "    \n",
    "    # Vehicle-level metrics\n",
    "    vehicle_metrics = {}\n",
    "    unique_vehicles = np.unique(id_idx_val)\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        mask = (id_idx_val == veh_id)\n",
    "        if np.sum(mask) > 0:\n",
    "            veh_speed_true = label_val[mask]\n",
    "            veh_speed_pred = mean_speed_pred[mask]\n",
    "            veh_accel_true = real_acceleration[mask]\n",
    "            veh_accel_pred = mean_acceleration_pred[mask]\n",
    "            veh_spacing_true = s_val[mask]\n",
    "            veh_spacing_pred = np.mean(all_samples_spacing_predictions[:, mask], axis=0)\n",
    "            \n",
    "            vehicle_metrics[veh_id] = {\n",
    "                'speed': calculate_metrics(veh_speed_true, veh_speed_pred),\n",
    "                'acceleration': calculate_metrics(veh_accel_true, veh_accel_pred),\n",
    "                'spacing': calculate_metrics(veh_spacing_true, veh_spacing_pred)\n",
    "            }\n",
    "    \n",
    "    # Extract individual parameters for analysis\n",
    "    individual_params = parameters_samples.reshape(-1, D)\n",
    "    \n",
    "    validation_results = {\n",
    "        'all_samples_speed_predictions': all_samples_speed_predictions,\n",
    "        'all_samples_spacing_predictions': all_samples_spacing_predictions,\n",
    "        'all_samples_acceleration_predictions': all_samples_acceleration_predictions,\n",
    "        'real_acceleration': real_acceleration,\n",
    "        'valid_indices': valid_indices,\n",
    "        'n_samples': n_samples,\n",
    "        'speed_metrics': speed_metrics,\n",
    "        'acceleration_metrics': acceleration_metrics,\n",
    "        'spacing_metrics': spacing_metrics,\n",
    "        'vehicle_metrics': vehicle_metrics,\n",
    "        'individual_params': individual_params,\n",
    "        'mean_predictions': {\n",
    "            'speed': mean_speed_pred,\n",
    "            'acceleration': mean_acceleration_pred,\n",
    "            'spacing': np.mean(all_samples_spacing_predictions, axis=0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Hierarchical IDM model validation completed!\")\n",
    "    return validation_results\n",
    "\n",
    "def plot_hierarchical_idm_validation_results(val_data, validation_results):\n",
    "    \"\"\"\n",
    "    Plot comprehensive validation results for hierarchical IDM model\n",
    "    \"\"\"\n",
    "    vt_val = val_data['vt']\n",
    "    s_val = val_data['s']\n",
    "    label_val = val_data['label_v']\n",
    "    id_idx_val = val_data['id_idx']\n",
    "    \n",
    "    all_samples_speed = validation_results['all_samples_speed_predictions']\n",
    "    all_samples_spacing = validation_results['all_samples_spacing_predictions']\n",
    "    all_samples_acceleration = validation_results['all_samples_acceleration_predictions']\n",
    "    real_acceleration = validation_results['real_acceleration']\n",
    "    n_samples = validation_results['n_samples']\n",
    "    \n",
    "    unique_vehicles = np.unique(id_idx_val)\n",
    "    n_vehicles = len(unique_vehicles)\n",
    "    \n",
    "    # Create comprehensive subplots for each vehicle\n",
    "    fig, axes = plt.subplots(n_vehicles, 3, figsize=(20, 5*n_vehicles))\n",
    "    if n_vehicles == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, veh_id in enumerate(unique_vehicles):\n",
    "        mask = (id_idx_val == veh_id)\n",
    "        if np.sum(mask) > 0:\n",
    "            # Ensure we don't exceed array bounds\n",
    "            veh_data_points = min(np.sum(mask), len(all_samples_speed[0]))\n",
    "            time_points = np.arange(veh_data_points)\n",
    "            \n",
    "            # ========== Speed Visualization ==========\n",
    "            axes[idx, 0].plot(time_points, label_val[mask][:veh_data_points], 'k-', \n",
    "                             label='True Speed', linewidth=3, alpha=0.9)\n",
    "            \n",
    "            # Extract posterior samples for this vehicle\n",
    "            veh_speed_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_speed])\n",
    "            \n",
    "            # Calculate confidence intervals\n",
    "            lower_5 = np.percentile(veh_speed_samples, 5, axis=0)\n",
    "            upper_95 = np.percentile(veh_speed_samples, 95, axis=0)\n",
    "            lower_25 = np.percentile(veh_speed_samples, 25, axis=0)\n",
    "            upper_75 = np.percentile(veh_speed_samples, 75, axis=0)\n",
    "            \n",
    "            # Plot confidence intervals\n",
    "            axes[idx, 0].fill_between(time_points, lower_5, upper_95, \n",
    "                                     alpha=0.3, color='red', label='90% CI')\n",
    "            axes[idx, 0].fill_between(time_points, lower_25, upper_75, \n",
    "                                     alpha=0.5, color='red', label='50% CI')\n",
    "            \n",
    "            # Plot mean and median\n",
    "            mean_speed = np.mean(veh_speed_samples, axis=0)\n",
    "            median_speed = np.median(veh_speed_samples, axis=0)\n",
    "            \n",
    "            axes[idx, 0].plot(time_points, mean_speed, 'b-', \n",
    "                             label='Mean Prediction', linewidth=2, alpha=0.8)\n",
    "            axes[idx, 0].plot(time_points, median_speed, 'g--', \n",
    "                             label='Median Prediction', linewidth=2, alpha=0.8)\n",
    "            \n",
    "            # Plot some sample trajectories\n",
    "            sample_indices_to_plot = np.random.choice(len(all_samples_speed), \n",
    "                                                     min(10, len(all_samples_speed)), replace=False)\n",
    "            for sample_idx in sample_indices_to_plot:\n",
    "                axes[idx, 0].plot(time_points, all_samples_speed[sample_idx][mask][:veh_data_points], \n",
    "                                 'r-', alpha=0.2, linewidth=0.8)\n",
    "            \n",
    "            axes[idx, 0].set_title(f'Vehicle {veh_id} - Speed Prediction\\n({n_samples} Posterior Samples)', fontsize=12)\n",
    "            axes[idx, 0].set_xlabel('Time Index')\n",
    "            axes[idx, 0].set_ylabel('Speed (m/s)')\n",
    "            axes[idx, 0].legend(loc='upper right', fontsize=8)\n",
    "            axes[idx, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # ========== Spacing Visualization ==========\n",
    "            axes[idx, 1].plot(time_points, s_val[mask][:veh_data_points], 'k-', \n",
    "                             label='True Spacing', linewidth=3, alpha=0.9)\n",
    "            \n",
    "            veh_spacing_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_spacing])\n",
    "            \n",
    "            # Calculate spacing confidence intervals\n",
    "            lower_5_s = np.percentile(veh_spacing_samples, 5, axis=0)\n",
    "            upper_95_s = np.percentile(veh_spacing_samples, 95, axis=0)\n",
    "            lower_25_s = np.percentile(veh_spacing_samples, 25, axis=0)\n",
    "            upper_75_s = np.percentile(veh_spacing_samples, 75, axis=0)\n",
    "            \n",
    "            axes[idx, 1].fill_between(time_points, lower_5_s, upper_95_s, \n",
    "                                     alpha=0.3, color='magenta', label='90% CI')\n",
    "            axes[idx, 1].fill_between(time_points, lower_25_s, upper_75_s, \n",
    "                                     alpha=0.5, color='magenta', label='50% CI')\n",
    "            \n",
    "            mean_spacing = np.mean(veh_spacing_samples, axis=0)\n",
    "            median_spacing = np.median(veh_spacing_samples, axis=0)\n",
    "            \n",
    "            axes[idx, 1].plot(time_points, mean_spacing, 'g-', \n",
    "                             label='Mean Prediction', linewidth=2, alpha=0.8)\n",
    "            axes[idx, 1].plot(time_points, median_spacing, 'c--', \n",
    "                             label='Median Prediction', linewidth=2, alpha=0.8)\n",
    "            \n",
    "            # Plot sample trajectories\n",
    "            for sample_idx in sample_indices_to_plot:\n",
    "                axes[idx, 1].plot(time_points, all_samples_spacing[sample_idx][mask][:veh_data_points], \n",
    "                                 'm-', alpha=0.2, linewidth=0.8)\n",
    "            \n",
    "            axes[idx, 1].set_title(f'Vehicle {veh_id} - Spacing Prediction\\n({n_samples} Posterior Samples)', fontsize=12)\n",
    "            axes[idx, 1].set_xlabel('Time Index')\n",
    "            axes[idx, 1].set_ylabel('Spacing (m)')\n",
    "            axes[idx, 1].legend(loc='upper right', fontsize=8)\n",
    "            axes[idx, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # ========== Acceleration Visualization ==========\n",
    "            axes[idx, 2].plot(time_points, real_acceleration[mask][:veh_data_points], 'k-', \n",
    "                             label='True Acceleration', linewidth=3, alpha=0.9)\n",
    "            \n",
    "            veh_accel_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_acceleration])\n",
    "            \n",
    "            # Calculate acceleration confidence intervals\n",
    "            lower_5_a = np.percentile(veh_accel_samples, 5, axis=0)\n",
    "            upper_95_a = np.percentile(veh_accel_samples, 95, axis=0)\n",
    "            lower_25_a = np.percentile(veh_accel_samples, 25, axis=0)\n",
    "            upper_75_a = np.percentile(veh_accel_samples, 75, axis=0)\n",
    "            \n",
    "            axes[idx, 2].fill_between(time_points, lower_5_a, upper_95_a, \n",
    "                                     alpha=0.3, color='orange', label='90% CI')\n",
    "            axes[idx, 2].fill_between(time_points, lower_25_a, upper_75_a, \n",
    "                                     alpha=0.5, color='orange', label='50% CI')\n",
    "            \n",
    "            mean_acceleration = np.mean(veh_accel_samples, axis=0)\n",
    "            median_acceleration = np.median(veh_accel_samples, axis=0)\n",
    "            \n",
    "            axes[idx, 2].plot(time_points, mean_acceleration, 'c-', \n",
    "                             label='Mean Prediction', linewidth=2, alpha=0.8)\n",
    "            axes[idx, 2].plot(time_points, median_acceleration, 'y--', \n",
    "                             label='Median Prediction', linewidth=2, alpha=0.8)\n",
    "            \n",
    "            # Plot sample trajectories\n",
    "            for sample_idx in sample_indices_to_plot:\n",
    "                axes[idx, 2].plot(time_points, all_samples_acceleration[sample_idx][mask][:veh_data_points], \n",
    "                                 'y-', alpha=0.2, linewidth=0.8)\n",
    "            \n",
    "            axes[idx, 2].set_title(f'Vehicle {veh_id} - Acceleration Prediction\\n({n_samples} Posterior Samples)', fontsize=12)\n",
    "            axes[idx, 2].set_xlabel('Time Index')\n",
    "            axes[idx, 2].set_ylabel('Acceleration (m/s²)')\n",
    "            axes[idx, 2].legend(loc='upper right', fontsize=8)\n",
    "            axes[idx, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Hierarchical IDM Model - Comprehensive Validation Results', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot parameter distributions\n",
    "    plot_hierarchical_idm_parameter_distributions(validation_results)\n",
    "    \n",
    "    # Plot uncertainty analysis\n",
    "    plot_hierarchical_idm_uncertainty_analysis(val_data, validation_results)\n",
    "\n",
    "def plot_hierarchical_idm_parameter_distributions(validation_results):\n",
    "    \"\"\"\n",
    "    Plot posterior parameter distributions for hierarchical IDM model\n",
    "    \"\"\"\n",
    "    individual_params = validation_results['individual_params']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # IDM parameter posterior distributions\n",
    "    param_names = ['vmax', 'dsafe', 'tsafe', 'amax', 'amin']\n",
    "    param_scales = [25, 2, 1.6, 1.5, 1.5]  # Scaling factors\n",
    "    \n",
    "    for i in range(5):\n",
    "        row, col = i // 3, i % 3\n",
    "        scaled_params = individual_params[:, i] * param_scales[i]\n",
    "        axes[row, col].hist(scaled_params, bins=30, alpha=0.7, color='skyblue', \n",
    "                           edgecolor='black', density=True)\n",
    "        axes[row, col].axvline(np.mean(scaled_params), color='red', linestyle='--', \n",
    "                              label=f'Mean: {np.mean(scaled_params):.3f}')\n",
    "        axes[row, col].axvline(np.median(scaled_params), color='green', linestyle='--', \n",
    "                              label=f'Median: {np.median(scaled_params):.3f}')\n",
    "        axes[row, col].set_title(f'Posterior: {param_names[i]}', fontsize=12)\n",
    "        axes[row, col].set_xlabel('Parameter Value')\n",
    "        axes[row, col].set_ylabel('Density')\n",
    "        axes[row, col].legend(fontsize=8)\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add parameter correlations plot\n",
    "    if individual_params.shape[0] > 1:\n",
    "        # Show correlation between vmax and amax\n",
    "        vmax_scaled = individual_params[:, 0] * param_scales[0]\n",
    "        amax_scaled = individual_params[:, 3] * param_scales[3]\n",
    "        \n",
    "        axes[1, 2].scatter(vmax_scaled, amax_scaled, alpha=0.6, color='purple')\n",
    "        axes[1, 2].set_xlabel('vmax (m/s)')\n",
    "        axes[1, 2].set_ylabel('amax (m/s²)')\n",
    "        axes[1, 2].set_title('Parameter Correlation: vmax vs amax', fontsize=12)\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate and display correlation coefficient\n",
    "        correlation = np.corrcoef(vmax_scaled, amax_scaled)[0, 1]\n",
    "        axes[1, 2].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                       transform=axes[1, 2].transAxes, fontsize=10,\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Hierarchical IDM - Posterior Parameter Distributions', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "def plot_hierarchical_idm_uncertainty_analysis(val_data, validation_results):\n",
    "    \"\"\"\n",
    "    Plot uncertainty analysis for hierarchical IDM model\n",
    "    \"\"\"\n",
    "    all_samples_speed = validation_results['all_samples_speed_predictions']\n",
    "    all_samples_spacing = validation_results['all_samples_spacing_predictions']\n",
    "    all_samples_acceleration = validation_results['all_samples_acceleration_predictions']\n",
    "    n_samples = validation_results['n_samples']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Calculate uncertainty statistics for all vehicles\n",
    "    all_uncertainty_speed = []\n",
    "    all_uncertainty_spacing = []\n",
    "    all_uncertainty_acceleration = []\n",
    "    \n",
    "    unique_vehicles = np.unique(val_data['id_idx'])\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        mask = (val_data['id_idx'] == veh_id)\n",
    "        if np.sum(mask) > 0:\n",
    "            # Speed uncertainty (90% interval width)\n",
    "            veh_speed_samples = np.array([sample[mask] for sample in all_samples_speed])\n",
    "            speed_intervals = np.percentile(veh_speed_samples, 95, axis=0) - np.percentile(veh_speed_samples, 5, axis=0)\n",
    "            all_uncertainty_speed.extend(speed_intervals)\n",
    "            \n",
    "            # Spacing uncertainty\n",
    "            veh_spacing_samples = np.array([sample[mask] for sample in all_samples_spacing])\n",
    "            spacing_intervals = np.percentile(veh_spacing_samples, 95, axis=0) - np.percentile(veh_spacing_samples, 5, axis=0)\n",
    "            all_uncertainty_spacing.extend(spacing_intervals)\n",
    "            \n",
    "            # Acceleration uncertainty\n",
    "            veh_accel_samples = np.array([sample[mask] for sample in all_samples_acceleration])\n",
    "            accel_intervals = np.percentile(veh_accel_samples, 95, axis=0) - np.percentile(veh_accel_samples, 5, axis=0)\n",
    "            all_uncertainty_acceleration.extend(accel_intervals)\n",
    "    \n",
    "    # Plot uncertainty distributions\n",
    "    uncertainty_data = [\n",
    "        (all_uncertainty_speed, 'Speed Uncertainty (90% CI Width)', 'lightblue', 'm/s'),\n",
    "        (all_uncertainty_spacing, 'Spacing Uncertainty (90% CI Width)', 'lightgreen', 'm'),\n",
    "        (all_uncertainty_acceleration, 'Acceleration Uncertainty (90% CI Width)', 'lightyellow', 'm/s²')\n",
    "    ]\n",
    "    \n",
    "    for i, (data, title, color, unit) in enumerate(uncertainty_data):\n",
    "        if i < 3:\n",
    "            row, col = i // 2, i % 2\n",
    "            axes[row, col].hist(data, bins=30, alpha=0.7, color=color, edgecolor='black')\n",
    "            axes[row, col].axvline(np.mean(data), color='red', linestyle='--', \n",
    "                                 label=f'Mean: {np.mean(data):.3f} {unit}')\n",
    "            axes[row, col].axvline(np.median(data), color='blue', linestyle='--', \n",
    "                                 label=f'Median: {np.median(data):.3f} {unit}')\n",
    "            axes[row, col].set_title(title, fontsize=12)\n",
    "            axes[row, col].set_xlabel(f'Uncertainty ({unit})')\n",
    "            axes[row, col].set_ylabel('Frequency')\n",
    "            axes[row, col].legend(fontsize=8)\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot uncertainty over time for first vehicle\n",
    "    if len(unique_vehicles) > 0:\n",
    "        first_vehicle_mask = (val_data['id_idx'] == unique_vehicles[0])\n",
    "        veh_speed_samples = np.array([sample[first_vehicle_mask] for sample in all_samples_speed])\n",
    "        \n",
    "        time_points = np.arange(min(100, np.sum(first_vehicle_mask)))\n",
    "        lower_5 = np.percentile(veh_speed_samples[:, :len(time_points)], 5, axis=0)\n",
    "        upper_95 = np.percentile(veh_speed_samples[:, :len(time_points)], 95, axis=0)\n",
    "        mean_speed = np.mean(veh_speed_samples[:, :len(time_points)], axis=0)\n",
    "        \n",
    "        axes[1, 1].fill_between(time_points, lower_5, upper_95, alpha=0.3, color='red', label='90% CI')\n",
    "        axes[1, 1].plot(time_points, mean_speed, 'b-', label='Mean Prediction', linewidth=2)\n",
    "        axes[1, 1].plot(time_points, val_data['label_v'][first_vehicle_mask][:len(time_points)], \n",
    "                       'k-', label='True Speed', linewidth=2, alpha=0.8)\n",
    "        axes[1, 1].set_title(f'Vehicle {unique_vehicles[0]} - Speed Uncertainty Over Time', fontsize=12)\n",
    "        axes[1, 1].set_xlabel('Time Index')\n",
    "        axes[1, 1].set_ylabel('Speed (m/s)')\n",
    "        axes[1, 1].legend(fontsize=8)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Hierarchical IDM - Uncertainty Analysis', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "def print_hierarchical_idm_validation_summary(validation_results):\n",
    "    \"\"\"\n",
    "    Print comprehensive summary of hierarchical IDM validation results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HIERARCHICAL IDM MODEL VALIDATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    speed_metrics = validation_results['speed_metrics']\n",
    "    acceleration_metrics = validation_results['acceleration_metrics']\n",
    "    spacing_metrics = validation_results['spacing_metrics']\n",
    "    vehicle_metrics = validation_results['vehicle_metrics']\n",
    "    \n",
    "    print(f\"\\nOVERALL PERFORMANCE METRICS:\")\n",
    "    print(f\"Speed Prediction:\")\n",
    "    print(f\"  - RMSE: {speed_metrics['rmse']:.4f} m/s\")\n",
    "    print(f\"  - MAE: {speed_metrics['mae']:.4f} m/s\")\n",
    "    print(f\"  - NRMSE: {speed_metrics['nrmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nAcceleration Prediction:\")\n",
    "    print(f\"  - RMSE: {acceleration_metrics['rmse']:.4f} m/s²\")\n",
    "    print(f\"  - MAE: {acceleration_metrics['mae']:.4f} m/s²\")\n",
    "    print(f\"  - NRMSE: {acceleration_metrics['nrmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSpacing Prediction:\")\n",
    "    print(f\"  - RMSE: {spacing_metrics['rmse']:.4f} m\")\n",
    "    print(f\"  - MAE: {spacing_metrics['mae']:.4f} m\")\n",
    "    print(f\"  - NRMSE: {spacing_metrics['nrmse']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nVEHICLE-LEVEL PERFORMANCE:\")\n",
    "    for veh_id, metrics in vehicle_metrics.items():\n",
    "        print(f\"\\nVehicle {veh_id}:\")\n",
    "        print(f\"  Speed - RMSE: {metrics['speed']['rmse']:.4f} m/s, MAE: {metrics['speed']['mae']:.4f} m/s\")\n",
    "        print(f\"  Acceleration - RMSE: {metrics['acceleration']['rmse']:.4f} m/s², MAE: {metrics['acceleration']['mae']:.4f} m/s²\")\n",
    "        print(f\"  Spacing - RMSE: {metrics['spacing']['rmse']:.4f} m, MAE: {metrics['spacing']['mae']:.4f} m\")\n",
    "    \n",
    "    # Parameter statistics\n",
    "    individual_params = validation_results['individual_params']\n",
    "    param_names = ['vmax', 'dsafe', 'tsafe', 'amax', 'amin']\n",
    "    param_scales = [25, 2, 1.6, 1.5, 1.5]\n",
    "    \n",
    "    print(f\"\\nPARAMETER POSTERIOR STATISTICS:\")\n",
    "    for i, name in enumerate(param_names):\n",
    "        scaled_params = individual_params[:, i] * param_scales[i]\n",
    "        print(f\"  {name}: Mean = {np.mean(scaled_params):.3f}, Std = {np.std(scaled_params):.3f}, \"\n",
    "              f\"95% CI = [{np.percentile(scaled_params, 2.5):.3f}, {np.percentile(scaled_params, 97.5):.3f}]\")\n",
    "\n",
    "def run_hierarchical_idm_calibration_only(ar_idm_data):\n",
    "    \"\"\"\n",
    "    Run only hierarchical IDM model calibration\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HIERARCHICAL IDM MODEL CALIBRATION ONLY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Data splitting\n",
    "    print(\"\\nStep 1: Data Splitting\")\n",
    "    train_data, val_data = split_data_for_ar_idm(ar_idm_data, train_ratio=0.7)\n",
    "    \n",
    "    # Model training\n",
    "    print(\"\\nStep 2: Model Training\")\n",
    "    trace, model = train_hierarchical_idm_model(train_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HIERARCHICAL IDM CALIBRATION COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'trace': trace,\n",
    "        'model': model,\n",
    "        'train_data': train_data,\n",
    "        'val_data': val_data\n",
    "    }\n",
    "\n",
    "def run_hierarchical_idm_validation_only(calibration_results, n_posterior_samples=100):\n",
    "    \"\"\"\n",
    "    Run only hierarchical IDM model validation (using calibrated model)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HIERARCHICAL IDM MODEL VALIDATION ONLY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    trace = calibration_results['trace']\n",
    "    model = calibration_results['model']\n",
    "    train_data = calibration_results['train_data']\n",
    "    val_data = calibration_results['val_data']\n",
    "    \n",
    "    # Model validation\n",
    "    print(\"\\nStep 1: Model Validation with Posterior Sampling\")\n",
    "    validation_results = validate_hierarchical_idm_model_comprehensive_improved(\n",
    "        trace, model, train_data, val_data, n_samples=n_posterior_samples\n",
    "    )\n",
    "    \n",
    "    # Enhanced visualization\n",
    "    print(\"\\nStep 2: Enhanced Results Visualization\")\n",
    "    plot_hierarchical_idm_validation_results(val_data, validation_results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nStep 3: Performance Summary\")\n",
    "    print_hierarchical_idm_validation_summary(validation_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HIERARCHICAL IDM VALIDATION COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return validation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184b7fa-12ec-440b-8d5a-d67b4053505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_results = run_hierarchical_idm_calibration_only(ar_idm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ee1c7-49ab-4fc6-9e20-15c531106280",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = run_hierarchical_idm_validation_only(calibration_results, n_posterior_samples=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
