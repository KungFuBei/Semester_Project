{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1cbef-d309-4011-8870-fd876169cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ar_model(train_data, d=1, step=1):\n",
    "    \"\"\"\n",
    "    Train AR model on training set with individual AR coefficients for each vehicle\n",
    "    \"\"\"\n",
    "    print(\"Starting AR model training with individual AR coefficients...\")\n",
    "    \n",
    "    vt = train_data['vt']\n",
    "    s = train_data['s']\n",
    "    dv = train_data['dv']\n",
    "    label_v = train_data['label_v']\n",
    "    id_idx = train_data['id_idx']\n",
    "    N_veh = train_data['n_vehicles']\n",
    "    \n",
    "    dt = 0.5\n",
    "    D = 5\n",
    "    DELTA = 4\n",
    "    \n",
    "    coords = {\n",
    "        \"veh_id\": np.arange(N_veh),\n",
    "        \"ar_lag\": np.arange(d),\n",
    "        \"parameter\": np.arange(D)\n",
    "    }\n",
    "    \n",
    "    with pm.Model(coords=coords) as constant_ar_model:\n",
    "        chol, _, _ = pm.LKJCholeskyCov('chol', n=D, eta=2.0, \n",
    "                                      sd_dist=pm.Exponential.dist(2, shape=D))\n",
    "        \n",
    "        log_mu_vmax = pm.Normal('log_mu_vmax', mu=0, sigma=0.1)\n",
    "        log_mu_dsafe = pm.Normal('log_mu_dsafe', mu=0, sigma=0.1)\n",
    "        log_mu_tsafe = pm.Normal('log_mu_tsafe', mu=0, sigma=0.1)\n",
    "        log_mu_amax = pm.Normal('log_mu_amax', mu=0, sigma=0.1)\n",
    "        \n",
    "        log_ratio_mu = pm.Normal('log_ratio_mu', mu=np.log(0.5), sigma=0.1)\n",
    "        ratio_mu = pm.Deterministic('ratio_mu', pt.exp(log_ratio_mu))\n",
    "        \n",
    "        log_ratio_raw = pm.Normal('log_ratio_raw', mu=0, sigma=0.1, shape=N_veh)\n",
    "        ratio_individual = pm.Deterministic('ratio_individual', ratio_mu * pt.exp(log_ratio_raw))\n",
    "        \n",
    "        vals_raw_vmax = pm.Normal('vals_raw_vmax', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_dsafe = pm.Normal('vals_raw_dsafe', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_tsafe = pm.Normal('vals_raw_tsafe', mu=0, sigma=0.1, shape=N_veh)\n",
    "        vals_raw_amax = pm.Normal('vals_raw_amax', mu=0, sigma=0.1, shape=N_veh)\n",
    "        \n",
    "        vals_raw_first4 = pm.Deterministic('vals_raw_first4', pt.stack([\n",
    "            vals_raw_vmax, vals_raw_dsafe, vals_raw_tsafe, vals_raw_amax\n",
    "        ], axis=1))\n",
    "        \n",
    "        log_mu_first4 = pt.stack([log_mu_vmax, log_mu_dsafe, log_mu_tsafe, log_mu_amax])\n",
    "        log_parameters_first4 = pm.Deterministic('log_parameters_first4', \n",
    "                                               log_mu_first4 + pt.dot(vals_raw_first4, chol[:4, :4].T))\n",
    "        parameters_first4 = pm.Deterministic('parameters_first4', pt.exp(log_parameters_first4))\n",
    "        \n",
    "        amin_individual = ratio_individual * parameters_first4[:, 3]\n",
    "        \n",
    "        parameters = pm.Deterministic('parameters', pt.stack([\n",
    "            parameters_first4[:, 0],\n",
    "            parameters_first4[:, 1],\n",
    "            parameters_first4[:, 2],\n",
    "            parameters_first4[:, 3],\n",
    "            amin_individual\n",
    "        ], axis=1), dims=('veh_id', 'parameter'))\n",
    "        \n",
    "        s_a_list = []\n",
    "        s_v_list = []\n",
    "        \n",
    "        for i in range(N_veh):\n",
    "            s_a_i = pm.Exponential(f's_a_{i}', lam=2000)\n",
    "            s_v_i = pm.Exponential(f's_v_{i}', lam=4000)\n",
    "            s_a_list.append(s_a_i)\n",
    "            s_v_list.append(s_v_i)\n",
    "        \n",
    "        rho_mu = pm.Normal('rho_mu', mu=0., sigma=0.4, shape=d)\n",
    "        rho_raw = pm.Normal('rho_raw', mu=0, sigma=0.1, dims=(\"veh_id\", \"ar_lag\"))\n",
    "        rho = pm.Deterministic('rho', rho_mu + rho_raw, dims=(\"veh_id\", \"ar_lag\"))\n",
    "        \n",
    "        for i in range(N_veh):\n",
    "            mask = (id_idx == i)\n",
    "            if np.sum(mask) > (d * step) + 5:\n",
    "                s_veh = s[mask]\n",
    "                vt_veh = vt[mask]\n",
    "                dv_veh = dv[mask]\n",
    "                label_veh = label_v[mask]\n",
    "                \n",
    "                vmax = 25 * parameters[i, 0]\n",
    "                dsafe = 2 * parameters[i, 1]\n",
    "                tsafe = 1.6 * parameters[i, 2]\n",
    "                amax = 1.5 * parameters[i, 3]\n",
    "                amin = 1.5 * parameters[i, 4]\n",
    "                \n",
    "                sn = dsafe + vt_veh * tsafe + \\\n",
    "                     vt_veh * dv_veh / (2 * pm.math.sqrt(amax * amin))\n",
    "                a_idm = amax * (1 - (vt_veh / vmax) ** DELTA - (sn / s_veh) ** 2)\n",
    "                \n",
    "                mean_speed = vt_veh + a_idm * dt\n",
    "                \n",
    "                n = len(vt_veh)\n",
    "                if n > d:\n",
    "                    for lag in range(d):\n",
    "                        if n > lag + 1:\n",
    "                            vt_diff = vt_veh[lag+1:n] - vt_veh[lag:n-1]\n",
    "                            a_lag = a_idm[lag:n-1]\n",
    "                            \n",
    "                            ar_correction = rho[i, lag] * (vt_diff - a_lag * dt)\n",
    "                            \n",
    "                            start_idx = max(d, lag+1)\n",
    "                            if start_idx < n:\n",
    "                                correction_length = n - start_idx\n",
    "                                ar_slice = ar_correction[start_idx-lag-1:start_idx-lag-1+correction_length]\n",
    "                                mean_speed = pt.set_subtensor(\n",
    "                                    mean_speed[start_idx:n],\n",
    "                                    mean_speed[start_idx:n] + ar_slice\n",
    "                                )\n",
    "                \n",
    "                total_sigma = pm.math.sqrt((s_a_list[i] * dt) ** 2 + s_v_list[i] ** 2)\n",
    "                pm.Normal(f'obs_{i}', mu=mean_speed, \n",
    "                         sigma=total_sigma,\n",
    "                         observed=label_veh)\n",
    "\n",
    "        try:\n",
    "            trace = pm.sample(\n",
    "                draws=600, \n",
    "                tune=600, \n",
    "                random_seed=42, \n",
    "                chains=2,\n",
    "                target_accept=0.9, \n",
    "                return_inferencedata=True\n",
    "            )\n",
    "            print(\"AR model training with individual AR coefficients completed!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Sampling error: {e}\")\n",
    "            print(\"Trying more conservative sampling settings...\")\n",
    "            trace = pm.sample(\n",
    "                draws=1500, \n",
    "                tune=1000, \n",
    "                random_seed=42,\n",
    "                chains=2, \n",
    "                target_accept=0.8,\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "            print(\"AR model training with individual AR coefficients completed (using conservative settings)!\")\n",
    "    \n",
    "    return trace, constant_ar_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d927947-5489-44e2-bb9b-8d3f9071f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from collections import deque\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    nrmse = rmse / (np.max(y_true) - np.min(y_true))\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'nrmse': nrmse\n",
    "    }\n",
    "\n",
    "def robust_smooth_acceleration(velocity, dt=0.5, window_size=7, poly_order=2):\n",
    "    if len(velocity) < window_size:\n",
    "        acceleration = np.gradient(velocity, dt)\n",
    "        return acceleration\n",
    "    \n",
    "    try:\n",
    "        acceleration = signal.savitzky_golay(velocity, window_length=window_size, \n",
    "                                           polyorder=poly_order, deriv=1, delta=dt)\n",
    "        \n",
    "        if len(acceleration) > 10:\n",
    "            x_fit = np.arange(5) * dt\n",
    "            y_fit = acceleration[5:10]\n",
    "            if len(y_fit) >= 2:\n",
    "                slope, intercept = np.polyfit(x_fit[:len(y_fit)], y_fit, 1)\n",
    "                for i in range(5):\n",
    "                    acceleration[i] = intercept + slope * (i * dt)\n",
    "            \n",
    "            x_fit = np.arange(5) * dt\n",
    "            y_fit = acceleration[-10:-5]\n",
    "            if len(y_fit) >= 2:\n",
    "                slope, intercept = np.polyfit(x_fit[:len(y_fit)], y_fit, 1)\n",
    "                for i in range(5):\n",
    "                    acceleration[-(5-i)] = intercept + slope * ((4-i) * dt)\n",
    "        \n",
    "        return acceleration\n",
    "        \n",
    "    except:\n",
    "        acceleration = np.zeros_like(velocity)\n",
    "        for i in range(1, len(velocity)-1):\n",
    "            acceleration[i] = (velocity[i+1] - velocity[i-1]) / (2 * dt)\n",
    "        \n",
    "        if len(velocity) > 1:\n",
    "            acceleration[0] = (velocity[1] - velocity[0]) / dt\n",
    "            acceleration[-1] = (velocity[-1] - velocity[-2]) / dt\n",
    "        \n",
    "        window = min(5, len(acceleration))\n",
    "        acceleration = np.convolve(acceleration, np.ones(window)/window, mode='same')\n",
    "        \n",
    "        return acceleration\n",
    "\n",
    "def calculate_initial_acceleration(velocity, dt=0.5, method='savitzky_golay'):\n",
    "    if len(velocity) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    if method == 'savitzky_golay':\n",
    "        acc_all = robust_smooth_acceleration(velocity, dt)\n",
    "        return acc_all[0]\n",
    "    \n",
    "    elif method == 'robust_fit':\n",
    "        n_points = min(5, len(velocity))\n",
    "        t_points = np.arange(n_points) * dt\n",
    "        v_points = velocity[:n_points]\n",
    "        \n",
    "        slope, intercept = np.polyfit(t_points, v_points, 1)\n",
    "        return slope\n",
    "    \n",
    "    elif method == 'physical_constrained':\n",
    "        if len(velocity) >= 4:\n",
    "            weights = np.array([0.1, 0.2, 0.3, 0.4])[:len(velocity)]\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            t_points = np.arange(len(velocity)) * dt\n",
    "            A = np.vstack([t_points, np.ones(len(t_points))]).T\n",
    "            W = np.diag(weights)\n",
    "            slope, intercept = np.linalg.lstsq(A.T @ W @ A, A.T @ W @ velocity, rcond=None)[0]\n",
    "            return slope\n",
    "        else:\n",
    "            return (velocity[1] - velocity[0]) / dt\n",
    "    \n",
    "    else:\n",
    "        initial_acc = (velocity[1] - velocity[0]) / dt\n",
    "        return np.clip(initial_acc, -3.0, 3.0)\n",
    "\n",
    "def improved_robust_acceleration(velocity, dt=0.5, window_size=7, poly_order=2):\n",
    "    acceleration = robust_smooth_acceleration(velocity, dt, window_size, poly_order)\n",
    "    acceleration = np.clip(acceleration, -3.0, 3.0)\n",
    "    \n",
    "    if len(acceleration) > 10:\n",
    "        acceleration[:3] = np.mean(acceleration[:5])\n",
    "        acceleration[-3:] = np.mean(acceleration[-5:])\n",
    "    \n",
    "    return acceleration\n",
    "\n",
    "def split_data_for_ar_idm(ar_idm_data, train_ratio=0.7):\n",
    "    vt = ar_idm_data['vt']\n",
    "    s = ar_idm_data['s']\n",
    "    dv = ar_idm_data['dv']\n",
    "    label_v = ar_idm_data['label_v']\n",
    "    id_idx = ar_idm_data['id_idx']\n",
    "    \n",
    "    unique_vehicles = np.unique(id_idx)\n",
    "    \n",
    "    train_data = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    val_data = {\n",
    "        'vt': np.array([]),\n",
    "        's': np.array([]),\n",
    "        'dv': np.array([]),\n",
    "        'label_v': np.array([]),\n",
    "        'id_idx': np.array([], dtype=int),\n",
    "        'n_vehicles': ar_idm_data['n_vehicles'],\n",
    "        'tracks': {}\n",
    "    }\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        mask = (id_idx == veh_id)\n",
    "        n_points = np.sum(mask)\n",
    "        \n",
    "        if n_points < 20:\n",
    "            continue\n",
    "            \n",
    "        split_point = int(n_points * train_ratio)\n",
    "        \n",
    "        train_mask = np.zeros_like(mask, dtype=bool)\n",
    "        train_indices = np.where(mask)[0][:split_point]\n",
    "        train_mask[train_indices] = True\n",
    "        \n",
    "        val_mask = np.zeros_like(mask, dtype=bool)\n",
    "        val_indices = np.where(mask)[0][split_point:]\n",
    "        val_mask[val_indices] = True\n",
    "        \n",
    "        train_data['vt'] = np.concatenate([train_data['vt'], vt[train_mask]])\n",
    "        train_data['s'] = np.concatenate([train_data['s'], s[train_mask]])\n",
    "        train_data['dv'] = np.concatenate([train_data['dv'], dv[train_mask]])\n",
    "        train_data['label_v'] = np.concatenate([train_data['label_v'], label_v[train_mask]])\n",
    "        train_data['id_idx'] = np.concatenate([train_data['id_idx'], np.full(np.sum(train_mask), veh_id)])\n",
    "        \n",
    "        if np.sum(train_mask) > 0:\n",
    "            train_data['tracks'][veh_id] = {\n",
    "                'last_vt': vt[train_mask][-1],\n",
    "                'last_s': s[train_mask][-1],\n",
    "                'last_dv': dv[train_mask][-1] if len(dv[train_mask]) > 0 else 0.0\n",
    "            }\n",
    "        \n",
    "        val_data['vt'] = np.concatenate([val_data['vt'], vt[val_mask]])\n",
    "        val_data['s'] = np.concatenate([val_data['s'], s[val_mask]])\n",
    "        val_data['dv'] = np.concatenate([val_data['dv'], dv[val_mask]])\n",
    "        val_data['label_v'] = np.concatenate([val_data['label_v'], label_v[val_mask]])\n",
    "        val_data['id_idx'] = np.concatenate([val_data['id_idx'], np.full(np.sum(val_mask), veh_id)])\n",
    "    \n",
    "    print(f\"Training set: {len(train_data['vt'])} data points\")\n",
    "    print(f\"Validation set: {len(val_data['vt'])} data points\")\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62624a79-854a-45f2-ac5d-7e500c036ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ar_model_comprehensive_improved(trace, model, train_data, val_data, n_samples=50):\n",
    "    \"\"\"\n",
    "    Improved AR model validation with posterior sampling and proper boundary handling\n",
    "    Using individual AR coefficients for each vehicle\n",
    "    Adopting the same dual-correction logic as dynamic AR model\n",
    "    修改：前3步使用真实速度、加速度和间距进行warm-up\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting improved AR model validation with posterior sampling...\")\n",
    "    \n",
    "\n",
    "    n_chains = len(trace.posterior.chain)\n",
    "    n_draws = len(trace.posterior.draw)\n",
    "    \n",
    "\n",
    "    sample_indices = []\n",
    "    for _ in range(n_samples):\n",
    "        chain_idx = np.random.randint(0, n_chains)\n",
    "        draw_idx = np.random.randint(0, n_draws)\n",
    "        sample_indices.append((chain_idx, draw_idx))\n",
    "    \n",
    "    vt_val = val_data['vt']\n",
    "    s_val = val_data['s'] \n",
    "    dv_val = val_data['dv']\n",
    "    label_val = val_data['label_v']\n",
    "    id_idx_val = val_data['id_idx']\n",
    "    \n",
    "    dt = 0.5\n",
    "    DELTA = 4\n",
    "    d = 1  \n",
    "    \n",
    " \n",
    "    print(\"Calculating real acceleration using improved robust method...\")\n",
    "    real_acceleration_all = improved_robust_acceleration(vt_val, dt)\n",
    "    \n",
    "\n",
    "    all_samples_speed_predictions = []\n",
    "    all_samples_spacing_predictions = []\n",
    "    all_samples_acceleration_predictions = []\n",
    "    all_samples_ar_coefficients = [] \n",
    "    \n",
    " \n",
    "    all_samples_vehicle_predictions = {} \n",
    "    \n",
    "\n",
    "    for sample_idx, (chain_idx, draw_idx) in enumerate(sample_indices):\n",
    "        print(f\"Processing posterior sample {sample_idx + 1}/{n_samples}...\")\n",
    "        \n",
    "     \n",
    "        individual_params = trace.posterior['parameters'].sel(chain=chain_idx, draw=draw_idx).values\n",
    "       \n",
    "        ar_coeffs = trace.posterior['rho'].sel(chain=chain_idx, draw=draw_idx).values\n",
    "        \n",
    "        all_speed_predictions = []\n",
    "        all_spacing_predictions = []\n",
    "        all_acceleration_predictions = []\n",
    "        all_ar_coefficients_veh = []\n",
    "        \n",
    "\n",
    "        sample_vehicle_predictions = {}\n",
    "        \n",
    "        # Process each vehicle separately\n",
    "        unique_vehicles = np.unique(id_idx_val)\n",
    "        \n",
    "        for veh_id in unique_vehicles:\n",
    "            mask = (id_idx_val == veh_id)\n",
    "            if np.sum(mask) > d + 5:  # Ensure enough data points\n",
    "                vt_veh = vt_val[mask]\n",
    "                s_veh = s_val[mask]\n",
    "                dv_veh = dv_val[mask]\n",
    "                real_accel_veh = real_acceleration_all[mask]  \n",
    "                \n",
    "\n",
    "                # Fallback: use first validation point\n",
    "                initial_v = vt_veh[0]\n",
    "                initial_s = s_veh[0]\n",
    "                initial_dv = dv_veh[0] if len(dv_veh) > 0 else 0.0\n",
    "                \n",
    "                # Calculate improved initial acceleration from training data\n",
    "                if veh_id in train_data['tracks']:\n",
    "                    # Get the last few training points to calculate initial acceleration\n",
    "                    train_mask = (train_data['id_idx'] == veh_id)\n",
    "                    if np.sum(train_mask) >= 5:\n",
    "                        train_vt = train_data['vt'][train_mask]\n",
    "                        initial_acc = calculate_initial_acceleration(train_vt[-5:], dt, method='savitzky_golay')\n",
    "                    else:\n",
    "                        initial_acc = 0.0\n",
    "                else:\n",
    "                    initial_acc = 0.0\n",
    "                \n",
    "                # Apply physical constraints to initial acceleration\n",
    "                vmax = 25 * individual_params[veh_id, 0]\n",
    "                dsafe = 2 * individual_params[veh_id, 1]\n",
    "                tsafe = 1.6 * individual_params[veh_id, 2]\n",
    "                amax = 1.5 * individual_params[veh_id, 3]\n",
    "                amin = 1.5 * individual_params[veh_id, 4]\n",
    "                \n",
    "                # Limit initial acceleration in reasonable range\n",
    "                initial_acc = np.clip(initial_acc, -amin, amax)\n",
    "                \n",
    "              \n",
    "                warmup_steps = 5  \n",
    "                \n",
    "                # Initialize simulation arrays with proper initial conditions\n",
    "                v_sim = [initial_v]  # Use training final speed as initial\n",
    "                s_sim = [initial_s]   # Use training final spacing as initial\n",
    "                a_sim = [initial_acc]  # Improved initial acceleration from training data\n",
    "                \n",
    "   \n",
    "                v_history_real = deque([initial_v], maxlen=d+1)\n",
    "                \n",
    "  \n",
    "                idm_acc_history = deque([initial_acc], maxlen=d) \n",
    "\n",
    "                acceleration_errors = deque([], maxlen=d)\n",
    "                \n",
    "                n = len(vt_veh)\n",
    " \n",
    "                static_ar_coeffs = np.zeros((n, d))\n",
    "                for lag in range(d):\n",
    "                    static_ar_coeffs[:, lag] = ar_coeffs[veh_id, lag]  \n",
    "                \n",
    "                all_ar_coefficients_veh.append(static_ar_coeffs)\n",
    "                \n",
    "                # Simulate the entire trajectory with sequential AR correction\n",
    "                for i in range(n-1):\n",
    "                 \n",
    "                    if i < warmup_steps and i < len(vt_veh) - 1:\n",
    "                       \n",
    "                        v_next_real = vt_veh[i + 1] if (i + 1) < len(vt_veh) else vt_veh[i]\n",
    "                        s_next_real = s_veh[i + 1] if (i + 1) < len(s_veh) else s_veh[i]\n",
    "                        a_next_real = real_accel_veh[i] if i < len(real_accel_veh) else 0.0\n",
    "                        \n",
    "                      \n",
    "                        s_next_real = max(1.0, min(200.0, s_next_real))\n",
    "                        a_next_real = np.clip(a_next_real, -amin, amax)\n",
    "                \n",
    "                        v_sim.append(v_next_real)\n",
    "                        s_sim.append(s_next_real)\n",
    "                        a_sim.append(a_next_real)\n",
    "\n",
    "                        v_history_real.append(v_next_real)\n",
    " \n",
    "                        current_dv = dv_veh[i] if i < len(dv_veh) else 0.0\n",
    "                        sn = dsafe + v_sim[-2] * tsafe + v_sim[-2] * current_dv / (2 * np.sqrt(amax * amin))\n",
    "                        current_acc_idm = amax * (1 - (v_sim[-2] / vmax) ** DELTA - (sn / s_sim[-2]) ** 2)\n",
    "                        current_acc_idm = np.clip(current_acc_idm, -amin, amax)\n",
    "                        idm_acc_history.append(current_acc_idm)\n",
    "                        \n",
    "\n",
    "                        if i > 0:\n",
    "                            a_real_prev = real_accel_veh[i-1] if (i-1) < len(real_accel_veh) else 0.0\n",
    "                            prev_dv = dv_veh[i-1] if (i-1) < len(dv_veh) else 0.0\n",
    "                            prev_sn = dsafe + v_sim[-3] * tsafe + v_sim[-3] * prev_dv / (2 * np.sqrt(amax * amin))\n",
    "                            a_idm_prev = amax * (1 - (v_sim[-3] / vmax) ** DELTA - (prev_sn / s_sim[-3]) ** 2)\n",
    "                            a_idm_prev = np.clip(a_idm_prev, -amin, amax)\n",
    "                            a_error = a_real_prev - a_idm_prev\n",
    "                            acceleration_errors.append(a_error)\n",
    "                        \n",
    "                        is_warmup = True\n",
    "                    else:\n",
    "\n",
    "                        if i > 0:\n",
    "\n",
    "                            a_real_prev = real_accel_veh[i-1] if (i-1) < len(real_accel_veh) else 0.0\n",
    "                            \n",
    "\n",
    "                            prev_dv = dv_veh[i-1] if (i-1) < len(dv_veh) else 0.0\n",
    "                            prev_sn = dsafe + v_sim[-2] * tsafe + v_sim[-2] * prev_dv / (2 * np.sqrt(amax * amin))\n",
    "                            a_idm_prev = amax * (1 - (v_sim[-2] / vmax) ** DELTA - (prev_sn / s_sim[-2]) ** 2)\n",
    "                            a_idm_prev = np.clip(a_idm_prev, -amin, amax)\n",
    "                            \n",
    "\n",
    "                            a_error = a_real_prev - a_idm_prev\n",
    "                            acceleration_errors.append(a_error)\n",
    "                        \n",
    "\n",
    "                        current_dv = dv_veh[i] if i < len(dv_veh) else 0.0\n",
    "                        sn = dsafe + v_sim[-1] * tsafe + v_sim[-1] * current_dv / (2 * np.sqrt(amax * amin))\n",
    "                        current_acc_idm = amax * (1 - (v_sim[-1] / vmax) ** DELTA - (sn / s_sim[-1]) ** 2)\n",
    "                        current_acc_idm = np.clip(current_acc_idm, -amin, amax)\n",
    "                        \n",
    "\n",
    "                        v_next_base = v_sim[-1] + current_acc_idm * dt\n",
    "                        \n",
    "\n",
    "                        v_ar_correction = 0.0\n",
    "                        if len(v_history_real) > d:\n",
    "                            for lag in range(d):\n",
    "                                if len(v_history_real) > lag + 1:\n",
    "\n",
    "                                    vt_diff = v_history_real[-1] - v_history_real[-(lag+2)]\n",
    "                                    \n",
    " \n",
    "                                    if len(idm_acc_history) > lag:\n",
    "                                        a_lag = idm_acc_history[-(lag+1)]  \n",
    "                                    else:\n",
    "                                        a_lag = 0.0\n",
    "                                    \n",
    "\n",
    "                                    ar_coef = ar_coeffs[veh_id, lag]  # 静态AR系数\n",
    "                                    v_ar_correction += ar_coef * (vt_diff - a_lag * dt)\n",
    "                        \n",
    "\n",
    "                        v_next_after_v_correction = v_next_base + v_ar_correction\n",
    "                        \n",
    " \n",
    "                        a_ar_correction = 0.0\n",
    "                        if len(acceleration_errors) >= d:\n",
    "                            for lag in range(d):\n",
    "                                if len(acceleration_errors) > lag:\n",
    "                                    a_ar_correction += ar_coeffs[veh_id, lag] * acceleration_errors[-(lag+1)]\n",
    "                        \n",
    "\n",
    "                        current_acc_total = current_acc_idm + a_ar_correction\n",
    "                        current_acc_total = np.clip(current_acc_total, -amin, amax)\n",
    "                        \n",
    "\n",
    "                        if i + 1 < len(vt_veh):\n",
    "                            v_history_real.append(vt_veh[i + 1])\n",
    "                        else:\n",
    "                            v_history_real.append(v_next_after_v_correction)\n",
    "                        \n",
    "\n",
    "                        idm_acc_history.append(current_acc_idm)\n",
    "                        \n",
    "\n",
    "                        s_next = s_sim[-1] + (current_dv+0.5*v_ar_correction)* dt+0.5*current_acc_idm * dt* dt\n",
    "                        s_next = max(1.0, min(200.0, s_next))\n",
    "                        \n",
    "\n",
    "                        v_sim.append(v_next_after_v_correction)\n",
    "                        s_sim.append(s_next)\n",
    "                        a_sim.append(current_acc_total)\n",
    "                        \n",
    "                        is_warmup = False\n",
    "                \n",
    "                # Convert to arrays\n",
    "                ar_corrected_speed = np.array(v_sim)\n",
    "                corrected_spacing = np.array(s_sim)\n",
    "                corrected_acceleration = np.array(a_sim)\n",
    "                \n",
    "                all_speed_predictions.extend(ar_corrected_speed[:n])\n",
    "                all_spacing_predictions.extend(corrected_spacing[:n])\n",
    "                all_acceleration_predictions.extend(corrected_acceleration[:n])\n",
    "                \n",
    "\n",
    "                sample_vehicle_predictions[veh_id] = {\n",
    "                    'speed_pred': ar_corrected_speed[:n],\n",
    "                    'spacing_pred': corrected_spacing[:n],\n",
    "                    'acceleration_pred': corrected_acceleration[:n],\n",
    "                    'speed_true': vt_veh,\n",
    "                    'spacing_true': s_veh,\n",
    "                    'acceleration_true': real_accel_veh,\n",
    "                    'ar_coefficients': static_ar_coeffs,\n",
    "                    'warmup_steps': warmup_steps  \n",
    "                }\n",
    "                \n",
    "            else:\n",
    "                # Too few data points, use simple predictions\n",
    "                if veh_id in train_data['tracks']:\n",
    "                    initial_v = train_data['tracks'][veh_id]['last_vt']\n",
    "                    initial_s = train_data['tracks'][veh_id]['last_s']\n",
    "                else:\n",
    "                    initial_v = vt_veh[0] if len(vt_veh) > 0 else 0.0\n",
    "                    initial_s = s_veh[0] if len(s_veh) > 0 else 10.0\n",
    "                \n",
    "                simple_speed = np.full(len(vt_veh), initial_v)\n",
    "                simple_spacing = np.full(len(s_veh), initial_s)\n",
    "                simple_acceleration = np.zeros(len(vt_veh))\n",
    "                \n",
    "                all_speed_predictions.extend(simple_speed)\n",
    "                all_spacing_predictions.extend(simple_spacing)\n",
    "                all_acceleration_predictions.extend(simple_acceleration)\n",
    "                \n",
    "\n",
    "                empty_ar_coeffs = np.zeros((len(vt_veh), d))\n",
    "                all_ar_coefficients_veh.append(empty_ar_coeffs)\n",
    "                \n",
    "\n",
    "                sample_vehicle_predictions[veh_id] = {\n",
    "                    'speed_pred': simple_speed,\n",
    "                    'spacing_pred': simple_spacing,\n",
    "                    'acceleration_pred': simple_acceleration,\n",
    "                    'speed_true': vt_veh,\n",
    "                    'spacing_true': s_veh,\n",
    "                    'acceleration_true': real_accel_veh if len(real_accel_veh) == len(simple_speed) else np.zeros(len(simple_speed)),\n",
    "                    'ar_coefficients': empty_ar_coeffs,\n",
    "                    'warmup_steps': 0\n",
    "                }\n",
    "        \n",
    "        # Convert to arrays for this sample\n",
    "        all_speed_predictions = np.array(all_speed_predictions)\n",
    "        all_spacing_predictions = np.array(all_spacing_predictions)\n",
    "        all_acceleration_predictions = np.array(all_acceleration_predictions)\n",
    "        \n",
    "        all_samples_speed_predictions.append(all_speed_predictions)\n",
    "        all_samples_spacing_predictions.append(all_spacing_predictions)\n",
    "        all_samples_acceleration_predictions.append(all_acceleration_predictions)\n",
    "        all_samples_ar_coefficients.append(all_ar_coefficients_veh)\n",
    "        all_samples_vehicle_predictions[sample_idx] = sample_vehicle_predictions\n",
    "    \n",
    "\n",
    "    min_len = min(\n",
    "        len(all_samples_speed_predictions[0]), \n",
    "        len(all_samples_spacing_predictions[0]),\n",
    "        len(all_samples_acceleration_predictions[0]), \n",
    "        len(real_acceleration_all),\n",
    "        len(label_val), \n",
    "        len(s_val)\n",
    "    )\n",
    "    \n",
    "    boundary_cut = int(0.05 * min_len)\n",
    "    valid_indices = slice(boundary_cut, min_len - boundary_cut)\n",
    "    \n",
    "    print(f\"Excluding boundary segments: using indices {boundary_cut} to {min_len - boundary_cut}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-VEHICLE VALIDATION RESULTS (Using All Samples Average)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    vehicle_metrics = {}\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "\n",
    "        veh_speed_preds = []\n",
    "        veh_spacing_preds = []\n",
    "        veh_accel_preds = []\n",
    "        veh_speed_true = None\n",
    "        veh_spacing_true = None\n",
    "        veh_accel_true = None\n",
    "        \n",
    "        for sample_idx in range(n_samples):\n",
    "            if veh_id in all_samples_vehicle_predictions[sample_idx]:\n",
    "                veh_data = all_samples_vehicle_predictions[sample_idx][veh_id]\n",
    "                veh_speed_preds.append(veh_data['speed_pred'])\n",
    "                veh_spacing_preds.append(veh_data['spacing_pred'])\n",
    "                veh_accel_preds.append(veh_data['acceleration_pred'])\n",
    "                \n",
    "                if veh_speed_true is None:\n",
    "                    veh_speed_true = veh_data['speed_true']\n",
    "                    veh_spacing_true = veh_data['spacing_true']\n",
    "                    veh_accel_true = veh_data['acceleration_true']\n",
    "        \n",
    "        if veh_speed_true is not None and len(veh_speed_true) > 10:\n",
    "\n",
    "            avg_speed_pred = np.mean(veh_speed_preds, axis=0)\n",
    "            avg_spacing_pred = np.mean(veh_spacing_preds, axis=0)\n",
    "            avg_accel_pred = np.mean(veh_accel_preds, axis=0)\n",
    "            \n",
    "\n",
    "            veh_boundary_cut = int(0.05 * len(veh_speed_true))\n",
    "            veh_valid_indices = slice(veh_boundary_cut, len(veh_speed_true) - veh_boundary_cut)\n",
    "            \n",
    "\n",
    "            speed_metrics = calculate_metrics(\n",
    "                veh_speed_true[veh_valid_indices], \n",
    "                avg_speed_pred[veh_valid_indices]\n",
    "            )\n",
    "            spacing_metrics = calculate_metrics(\n",
    "                veh_spacing_true[veh_valid_indices], \n",
    "                avg_spacing_pred[veh_valid_indices]\n",
    "            )\n",
    "            acceleration_metrics = calculate_metrics(\n",
    "                veh_accel_true[veh_valid_indices],\n",
    "                avg_accel_pred[veh_valid_indices]\n",
    "            )\n",
    "            \n",
    "            vehicle_metrics[veh_id] = {\n",
    "                'speed': speed_metrics,\n",
    "                'spacing': spacing_metrics,\n",
    "                'acceleration': acceleration_metrics,\n",
    "                'n_points': len(veh_speed_true[veh_valid_indices])\n",
    "            }\n",
    "            \n",
    "\n",
    "            print(f\"\\nVehicle {veh_id} (n={vehicle_metrics[veh_id]['n_points']} points):\")\n",
    "            print(f\"  Speed - RMSE: {speed_metrics['rmse']:.4f} m/s, MAE: {speed_metrics['mae']:.4f} m/s, NRMSE: {speed_metrics['nrmse']:.4f}\")\n",
    "            print(f\"  Spacing - RMSE: {spacing_metrics['rmse']:.4f} m, MAE: {spacing_metrics['mae']:.4f} m, NRMSE: {spacing_metrics['nrmse']:.4f}\")\n",
    "            print(f\"  Acceleration - RMSE: {acceleration_metrics['rmse']:.4f} m/s², MAE: {acceleration_metrics['mae']:.4f} m/s², NRMSE: {acceleration_metrics['nrmse']:.4f}\")\n",
    "    \n",
    "\n",
    "    speed_metrics_all = []\n",
    "    spacing_metrics_all = []\n",
    "    acceleration_metrics_all = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        speed_metrics = calculate_metrics(\n",
    "            label_val[valid_indices], \n",
    "            all_samples_speed_predictions[i][valid_indices]\n",
    "        )\n",
    "        spacing_metrics = calculate_metrics(\n",
    "            s_val[valid_indices], \n",
    "            all_samples_spacing_predictions[i][valid_indices]\n",
    "        )\n",
    "        acceleration_metrics = calculate_metrics(\n",
    "            real_acceleration_all[valid_indices],  \n",
    "            all_samples_acceleration_predictions[i][valid_indices]\n",
    "        )\n",
    "        \n",
    "        speed_metrics_all.append(speed_metrics)\n",
    "        spacing_metrics_all.append(spacing_metrics)\n",
    "        acceleration_metrics_all.append(acceleration_metrics)\n",
    "    \n",
    "\n",
    "    avg_speed_metrics = {\n",
    "        'mse': np.mean([m['mse'] for m in speed_metrics_all]),\n",
    "        'rmse': np.mean([m['rmse'] for m in speed_metrics_all]),\n",
    "        'mae': np.mean([m['mae'] for m in speed_metrics_all]),\n",
    "        'nrmse': np.mean([m['nrmse'] for m in speed_metrics_all])\n",
    "    }\n",
    "    \n",
    "    avg_spacing_metrics = {\n",
    "        'mse': np.mean([m['mse'] for m in spacing_metrics_all]),\n",
    "        'rmse': np.mean([m['rmse'] for m in spacing_metrics_all]),\n",
    "        'mae': np.mean([m['mae'] for m in spacing_metrics_all]),\n",
    "        'nrmse': np.mean([m['nrmse'] for m in spacing_metrics_all])\n",
    "    }\n",
    "    \n",
    "    avg_acceleration_metrics = {\n",
    "        'mse': np.mean([m['mse'] for m in acceleration_metrics_all]),\n",
    "        'rmse': np.mean([m['rmse'] for m in acceleration_metrics_all]),\n",
    "        'mae': np.mean([m['mae'] for m in acceleration_metrics_all]),\n",
    "        'nrmse': np.mean([m['nrmse'] for m in acceleration_metrics_all])\n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VEHICLE PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if vehicle_metrics:\n",
    "        speed_rmse_values = [vm['speed']['rmse'] for vm in vehicle_metrics.values()]\n",
    "        spacing_rmse_values = [vm['spacing']['rmse'] for vm in vehicle_metrics.values()]\n",
    "        accel_rmse_values = [vm['acceleration']['rmse'] for vm in vehicle_metrics.values()]\n",
    "        \n",
    "        print(f\"Number of vehicles analyzed: {len(vehicle_metrics)}\")\n",
    "        print(f\"\\nSpeed RMSE - Mean: {np.mean(speed_rmse_values):.4f}, Std: {np.std(speed_rmse_values):.4f}, \"\n",
    "              f\"Min: {np.min(speed_rmse_values):.4f}, Max: {np.max(speed_rmse_values):.4f}\")\n",
    "        print(f\"Spacing RMSE - Mean: {np.mean(spacing_rmse_values):.4f}, Std: {np.std(spacing_rmse_values):.4f}, \"\n",
    "              f\"Min: {np.min(spacing_rmse_values):.4f}, Max: {np.max(spacing_rmse_values):.4f}\")\n",
    "        print(f\"Acceleration RMSE - Mean: {np.mean(accel_rmse_values):.4f}, Std: {np.std(accel_rmse_values):.4f}, \"\n",
    "              f\"Min: {np.min(accel_rmse_values):.4f}, Max: {np.max(accel_rmse_values):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OVERALL VALIDATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Improved AR Validation Results (using {n_samples} posterior samples, excluding boundaries):\")\n",
    "    print(f\"\\nSpeed Metrics:\")\n",
    "    print(f\"  RMSE: {avg_speed_metrics['rmse']:.4f} m/s\")\n",
    "    print(f\"  NRMSE: {avg_speed_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MAE: {avg_speed_metrics['mae']:.4f} m/s\")\n",
    "    \n",
    "    print(f\"\\nSpacing Metrics:\")\n",
    "    print(f\"  RMSE: {avg_spacing_metrics['rmse']:.4f} m\")\n",
    "    print(f\"  NRMSE: {avg_spacing_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MAE: {avg_spacing_metrics['mae']:.4f} m\")\n",
    "    \n",
    "    print(f\"\\nAcceleration Metrics:\")\n",
    "    print(f\"  RMSE: {avg_acceleration_metrics['rmse']:.4f} m/s²\")\n",
    "    print(f\"  NRMSE: {avg_acceleration_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MAE: {avg_acceleration_metrics['mae']:.4f} m/s²\")\n",
    "    \n",
    "    return {\n",
    "        'speed_metrics': avg_speed_metrics,\n",
    "        'spacing_metrics': avg_spacing_metrics,\n",
    "        'acceleration_metrics': avg_acceleration_metrics,\n",
    "        'vehicle_metrics': vehicle_metrics,  \n",
    "        'all_samples_speed_predictions': all_samples_speed_predictions,\n",
    "        'all_samples_spacing_predictions': all_samples_spacing_predictions,\n",
    "        'all_samples_acceleration_predictions': all_samples_acceleration_predictions,\n",
    "        'all_samples_ar_coefficients': all_samples_ar_coefficients,\n",
    "        'all_samples_vehicle_predictions': all_samples_vehicle_predictions,  \n",
    "        'real_acceleration': real_acceleration_all,  \n",
    "        'individual_params': trace.posterior['parameters'].mean(dim=(\"chain\", \"draw\")).values,\n",
    "        'ar_coeffs': trace.posterior['rho'].mean(dim=(\"chain\", \"draw\")).values,\n",
    "        'valid_indices': valid_indices,\n",
    "        'n_samples': n_samples,\n",
    "        'unique_vehicles': unique_vehicles,\n",
    "        'warmup_steps': warmup_steps \n",
    "    }\n",
    "\n",
    "\n",
    "def plot_comprehensive_validation_results_improved(val_data, validation_results):\n",
    "    \"\"\"\n",
    "    Plot improved comprehensive validation results with posterior samples\n",
    "    Enhanced uncertainty visualization for each vehicle\n",
    "    \"\"\"\n",
    "    vt_val = val_data['vt']\n",
    "    s_val = val_data['s']\n",
    "    label_val = val_data['label_v']\n",
    "    id_idx_val = val_data['id_idx']\n",
    "    \n",
    "    all_samples_speed = validation_results['all_samples_speed_predictions']\n",
    "    all_samples_spacing = validation_results['all_samples_spacing_predictions']\n",
    "    all_samples_acceleration = validation_results['all_samples_acceleration_predictions']\n",
    "    real_acceleration = validation_results['real_acceleration']\n",
    "    valid_indices = validation_results['valid_indices']\n",
    "    n_samples = validation_results['n_samples']\n",
    "    \n",
    "    unique_vehicles = np.unique(id_idx_val)\n",
    "    n_vehicles = len(unique_vehicles)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_vehicles, 3, figsize=(20, 5*n_vehicles))\n",
    "    if n_vehicles == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, veh_id in enumerate(unique_vehicles):\n",
    "        mask = (id_idx_val == veh_id)\n",
    "        if np.sum(mask) > 0:\n",
    "            veh_data_points = min(np.sum(mask), len(all_samples_speed[0]))\n",
    "            time_points = np.arange(veh_data_points)\n",
    "            \n",
    "            axes[idx, 0].plot(time_points, label_val[mask][:veh_data_points], 'k-', \n",
    "                             label='True Speed', linewidth=3, alpha=0.9)\n",
    "            \n",
    "            veh_speed_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_speed])\n",
    "            \n",
    "            lower_5 = np.percentile(veh_speed_samples, 5, axis=0)\n",
    "            upper_95 = np.percentile(veh_speed_samples, 95, axis=0)\n",
    "            lower_25 = np.percentile(veh_speed_samples, 25, axis=0)\n",
    "            upper_75 = np.percentile(veh_speed_samples, 75, axis=0)\n",
    "            \n",
    "            axes[idx, 0].fill_between(time_points, lower_5, upper_95, \n",
    "                                     alpha=0.3, color='red', label='90% CI')\n",
    "            axes[idx, 0].fill_between(time_points, lower_25, upper_75, \n",
    "                                     alpha=0.5, color='red', label='50% CI')\n",
    "            \n",
    "            mean_speed = np.mean(veh_speed_samples, axis=0)\n",
    "            median_speed = np.median(veh_speed_samples, axis=0)\n",
    "            \n",
    "            axes[idx, 0].plot(time_points, mean_speed, 'b-', \n",
    "                             label='Mean Prediction', linewidth=2, alpha=0.8)\n",
    "            axes[idx, 0].plot(time_points, median_speed, 'g--', \n",
    "                             label='Median Prediction', linewidth=2, alpha=0.8)\n",
    "            \n",
    "            sample_indices_to_plot = np.random.choice(len(all_samples_speed), \n",
    "                                                     min(10, len(all_samples_speed)), replace=False)\n",
    "            for sample_idx in sample_indices_to_plot:\n",
    "                axes[idx, 0].plot(time_points, all_samples_speed[sample_idx][mask][:veh_data_points], \n",
    "                                 'r-', alpha=0.2, linewidth=0.8)\n",
    "            \n",
    "            axes[idx, 0].set_title(f'Vehicle {veh_id} - Speed Prediction\\n({n_samples} Posterior Samples)', fontsize=12)\n",
    "            axes[idx, 0].set_xlabel('Time Index')\n",
    "            axes[idx, 0].set_ylabel('Speed (m/s)')\n",
    "            axes[idx, 0].legend(loc='upper right', fontsize=8)\n",
    "            axes[idx, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[idx, 1].plot(time_points, s_val[mask][:veh_data_points], 'k-', \n",
    "                             label='True Spacing', linewidth=3, alpha=0.9)\n",
    "            \n",
    "            veh_spacing_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_spacing])\n",
    "            \n",
    "            lower_5_s = np.percentile(veh_spacing_samples, 5, axis=0)\n",
    "            upper_95_s = np.percentile(veh_spacing_samples, 95, axis=0)\n",
    "            lower_25_s = np.percentile(veh_spacing_samples, 25, axis=0)\n",
    "            upper_75_s = np.percentile(veh_spacing_samples, 75, axis=0)\n",
    "            \n",
    "            axes[idx, 1].fill_between(time_points, lower_5_s, upper_95_s, \n",
    "                                     alpha=0.3, color='magenta', label='90% CI')\n",
    "            axes[idx, 1].fill_between(time_points, lower_25_s, upper_75_s, \n",
    "                                     alpha=0.5, color='magenta', label='50% CI')\n",
    "            \n",
    "            mean_spacing = np.mean(veh_spacing_samples, axis=0)\n",
    "            median_spacing = np.median(veh_spacing_samples, axis=0)\n",
    "            \n",
    "            axes[idx, 1].plot(time_points, mean_spacing, 'g-', \n",
    "                             label='Mean Prediction', linewidth=2, alpha=0.8)\n",
    "            axes[idx, 1].plot(time_points, median_spacing, 'c--', \n",
    "                             label='Median Prediction', linewidth=2, alpha=0.8)\n",
    "            \n",
    "            for sample_idx in sample_indices_to_plot:\n",
    "                axes[idx, 1].plot(time_points, all_samples_spacing[sample_idx][mask][:veh_data_points], \n",
    "                                 'm-', alpha=0.2, linewidth=0.8)\n",
    "            \n",
    "            axes[idx, 1].set_title(f'Vehicle {veh_id} - Spacing Prediction\\n({n_samples} Posterior Samples)', fontsize=12)\n",
    "            axes[idx, 1].set_xlabel('Time Index')\n",
    "            axes[idx, 1].set_ylabel('Spacing (m)')\n",
    "            axes[idx, 1].legend(loc='upper right', fontsize=8)\n",
    "            axes[idx, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[idx, 2].plot(time_points, real_acceleration[mask][:veh_data_points], 'k-', \n",
    "                             label='True Acceleration', linewidth=3, alpha=0.9)\n",
    "            \n",
    "            veh_accel_samples = np.array([sample[mask][:veh_data_points] for sample in all_samples_acceleration])\n",
    "            \n",
    "            lower_5_a = np.percentile(veh_accel_samples, 5, axis=0)\n",
    "            upper_95_a = np.percentile(veh_accel_samples, 95, axis=0)\n",
    "            lower_25_a = np.percentile(veh_accel_samples, 25, axis=0)\n",
    "            upper_75_a = np.percentile(veh_accel_samples, 75, axis=0)\n",
    "            \n",
    "            axes[idx, 2].fill_between(time_points, lower_5_a, upper_95_a, \n",
    "                                     alpha=0.3, color='orange', label='90% CI')\n",
    "            axes[idx, 2].fill_between(time_points, lower_25_a, upper_75_a, \n",
    "                                     alpha=0.5, color='orange', label='50% CI')\n",
    "            \n",
    "            mean_acceleration = np.mean(veh_accel_samples, axis=0)\n",
    "            median_acceleration = np.median(veh_accel_samples, axis=0)\n",
    "            \n",
    "            axes[idx, 2].plot(time_points, mean_acceleration, 'c-', \n",
    "                             label='Mean Prediction', linewidth=2, alpha=0.8)\n",
    "            axes[idx, 2].plot(time_points, median_acceleration, 'y--', \n",
    "                             label='Median Prediction', linewidth=2, alpha=0.8)\n",
    "            \n",
    "            for sample_idx in sample_indices_to_plot:\n",
    "                axes[idx, 2].plot(time_points, all_samples_acceleration[sample_idx][mask][:veh_data_points], \n",
    "                                 'y-', alpha=0.2, linewidth=0.8)\n",
    "            \n",
    "            axes[idx, 2].set_title(f'Vehicle {veh_id} - Acceleration Prediction\\n({n_samples} Posterior Samples)', fontsize=12)\n",
    "            axes[idx, 2].set_xlabel('Time Index')\n",
    "            axes[idx, 2].set_ylabel('Acceleration (m/s²)')\n",
    "            axes[idx, 2].legend(loc='upper right', fontsize=8)\n",
    "            axes[idx, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plot_posterior_parameter_distributions(validation_results)\n",
    "    plot_uncertainty_summary(val_data, validation_results)\n",
    "\n",
    "def plot_posterior_parameter_distributions(validation_results):\n",
    "    individual_params = validation_results['individual_params']\n",
    "    ar_coeffs = validation_results['ar_coeffs']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    param_names = ['vmax', 'dsafe', 'tsafe', 'amax', 'amin']\n",
    "    param_scales = [25, 2, 1.6, 1.5, 1.5]\n",
    "    \n",
    "    for i in range(5):\n",
    "        row, col = i // 3, i % 3\n",
    "        scaled_params = individual_params[:, i] * param_scales[i]\n",
    "        axes[row, col].hist(scaled_params, bins=20, alpha=0.7, color='skyblue', \n",
    "                           edgecolor='black', density=True)\n",
    "        axes[row, col].axvline(np.mean(scaled_params), color='red', linestyle='--', \n",
    "                              label=f'Mean: {np.mean(scaled_params):.3f}')\n",
    "        axes[row, col].set_title(f'Posterior: {param_names[i]}', fontsize=12)\n",
    "        axes[row, col].set_xlabel('Parameter Value')\n",
    "        axes[row, col].set_ylabel('Density')\n",
    "        axes[row, col].legend()\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    if len(ar_coeffs.shape) > 1 and ar_coeffs.shape[1] > 0:\n",
    "        axes[1, 2].hist(ar_coeffs[:, 0], bins=20, alpha=0.7, color='lightcoral', \n",
    "                       edgecolor='black', density=True)\n",
    "        axes[1, 2].axvline(np.mean(ar_coeffs[:, 0]), color='red', linestyle='--', \n",
    "                          label=f'Mean: {np.mean(ar_coeffs[:, 0]):.3f}')\n",
    "        axes[1, 2].set_title('Posterior: AR Coefficient 1', fontsize=12)\n",
    "        axes[1, 2].set_xlabel('AR Coefficient Value')\n",
    "        axes[1, 2].set_ylabel('Density')\n",
    "        axes[1, 2].legend()\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Posterior Parameter Distributions', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "def plot_uncertainty_summary(val_data, validation_results):\n",
    "    all_samples_speed = validation_results['all_samples_speed_predictions']\n",
    "    all_samples_spacing = validation_results['all_samples_spacing_predictions']\n",
    "    all_samples_acceleration = validation_results['all_samples_acceleration_predictions']\n",
    "    n_samples = validation_results['n_samples']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    all_uncertainty_speed = []\n",
    "    all_uncertainty_spacing = []\n",
    "    all_uncertainty_acceleration = []\n",
    "    \n",
    "    unique_vehicles = np.unique(val_data['id_idx'])\n",
    "    \n",
    "    for veh_id in unique_vehicles:\n",
    "        mask = (val_data['id_idx'] == veh_id)\n",
    "        if np.sum(mask) > 0:\n",
    "            veh_speed_samples = np.array([sample[mask] for sample in all_samples_speed])\n",
    "            speed_intervals = np.percentile(veh_speed_samples, 95, axis=0) - np.percentile(veh_speed_samples, 5, axis=0)\n",
    "            all_uncertainty_speed.extend(speed_intervals)\n",
    "            \n",
    "            veh_spacing_samples = np.array([sample[mask] for sample in all_samples_spacing])\n",
    "            spacing_intervals = np.percentile(veh_spacing_samples, 95, axis=0) - np.percentile(veh_spacing_samples, 5, axis=0)\n",
    "            all_uncertainty_spacing.extend(spacing_intervals)\n",
    "            \n",
    "            veh_accel_samples = np.array([sample[mask] for sample in all_samples_acceleration])\n",
    "            accel_intervals = np.percentile(veh_accel_samples, 95, axis=0) - np.percentile(veh_accel_samples, 5, axis=0)\n",
    "            all_uncertainty_acceleration.extend(accel_intervals)\n",
    "    \n",
    "    uncertainty_data = [\n",
    "        (all_uncertainty_speed, 'Speed Uncertainty (90% CI Width)', 'lightblue', 'm/s'),\n",
    "        (all_uncertainty_spacing, 'Spacing Uncertainty (90% CI Width)', 'lightgreen', 'm'),\n",
    "        (all_uncertainty_acceleration, 'Acceleration Uncertainty (90% CI Width)', 'lightyellow', 'm/s²')\n",
    "    ]\n",
    "    \n",
    "    for i, (data, title, color, unit) in enumerate(uncertainty_data):\n",
    "        if i < 3:\n",
    "            row, col = i // 2, i % 2\n",
    "            axes[row, col].hist(data, bins=30, alpha=0.7, color=color, edgecolor='black')\n",
    "            axes[row, col].axvline(np.mean(data), color='red', linestyle='--', \n",
    "                                 label=f'Mean: {np.mean(data):.3f} {unit}')\n",
    "            axes[row, col].set_title(title, fontsize=12)\n",
    "            axes[row, col].set_xlabel(f'Uncertainty ({unit})')\n",
    "            axes[row, col].set_ylabel('Frequency')\n",
    "            axes[row, col].legend()\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    if len(unique_vehicles) > 0:\n",
    "        first_vehicle_mask = (val_data['id_idx'] == unique_vehicles[0])\n",
    "        veh_speed_samples = np.array([sample[first_vehicle_mask] for sample in all_samples_speed])\n",
    "        \n",
    "        time_points = np.arange(min(100, np.sum(first_vehicle_mask)))\n",
    "        lower_5 = np.percentile(veh_speed_samples[:, :len(time_points)], 5, axis=0)\n",
    "        upper_95 = np.percentile(veh_speed_samples[:, :len(time_points)], 95, axis=0)\n",
    "        mean_speed = np.mean(veh_speed_samples[:, :len(time_points)], axis=0)\n",
    "        \n",
    "        axes[1, 1].fill_between(time_points, lower_5, upper_95, alpha=0.3, color='red', label='90% CI')\n",
    "        axes[1, 1].plot(time_points, mean_speed, 'b-', label='Mean Prediction', linewidth=2)\n",
    "        axes[1, 1].plot(time_points, val_data['label_v'][first_vehicle_mask][:len(time_points)], \n",
    "                       'k-', label='True Speed', linewidth=2, alpha=0.8)\n",
    "        axes[1, 1].set_title(f'Vehicle {unique_vehicles[0]} - Speed Uncertainty Over Time', fontsize=12)\n",
    "        axes[1, 1].set_xlabel('Time Index')\n",
    "        axes[1, 1].set_ylabel('Speed (m/s)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Uncertainty Analysis Summary', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "def run_calibration_only(ar_idm_data, ar_order=1):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"AR MODEL CALIBRATION ONLY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nStep 1: Data Splitting\")\n",
    "    train_data, val_data = split_data_for_ar_idm(ar_idm_data, train_ratio=0.7)\n",
    "    \n",
    "    print(\"\\nStep 2: Model Training\")\n",
    "    trace, model = train_ar_model(train_data, d=ar_order, step=1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CALIBRATION COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'trace': trace,\n",
    "        'model': model,\n",
    "        'train_data': train_data,\n",
    "        'val_data': val_data\n",
    "    }\n",
    "\n",
    "def run_validation_only(calibration_results, n_posterior_samples=100):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"AR MODEL VALIDATION ONLY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    trace = calibration_results['trace']\n",
    "    model = calibration_results['model']\n",
    "    train_data = calibration_results['train_data']\n",
    "    val_data = calibration_results['val_data']\n",
    "    \n",
    "    print(\"\\nStep 1: Model Validation with Posterior Sampling\")\n",
    "    validation_results = validate_ar_model_comprehensive_improved(\n",
    "        trace, model, train_data, val_data, n_samples=n_posterior_samples\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStep 2: Enhanced Results Visualization\")\n",
    "    plot_comprehensive_validation_results_improved(val_data, validation_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96047ecd-853f-45da-9155-d85e82fec2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_result = run_calibration_only(ar_idm_data, ar_order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d483bc-c0ee-4347-b4dd-9b2d174a8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result = run_validation_only(calibration_result, n_posterior_samples=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
